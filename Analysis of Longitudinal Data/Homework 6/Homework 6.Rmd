---
title: "Longitudinal Homework 6"
author: "Tim Vigers"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(MASS)
library(lme4)
library(nlme)
```

```{r data, echo=FALSE}
df <- read.csv("/Users/timvigers/Documents/GitHub/School/Analysis of Longitudinal Data/Homework 6/albuterol.csv")
```

# 1. Model planning

## a. Software

### i. Medication use

In order to adapt a generalized linear model (GzLM) for serially correlated data, I would use a generalized estimating equation (GEE). This could be fit using PROC GENMOD in SAS, with a Poisson distribution since this is count data. There is also a package called "geepack" in R that can fit GEEs, but I've never used it and don't know much about it, so PROC GENMOD is probably safer.

### ii. FEV1

I think you can probably get away with a normal theory model for FEV1 data, unless it's really skewed. If so, I would use either PROC MIXED in SAS or gls() in R. If normal theory models won't work, then I would use PROC GENMOD or geeglm() like above, to model the outcome with a non-normal distribution.

## b. Data

### i. Medication use

Because we need a GEE for this outcome, I would set up the data so that each subject has a row for every day during the relevant timeframe. On days without an albuterol count the outcome would be filled in as missing (NA in R), but the temporal spacing would be equal between rows within subject.

The SAS code would look something like:

```{sas eval=FALSE}
proc genmod data=albuterol;
class id friday;
model albuterol_use = 
  friday ln_mmax_pm25 temperature pressure humidity / solution dist=poisson;
repeated subject=id / TYPE = AR(1);
run;
```

### ii. FEV1

Again, assuming that a normal mixed model will work, the PROC MIXED code would be something like:

```{sas eval=FALSE}
proc mixed data = albuterol;
class id friday;
model fev1 = date friday ln_mmax_pm25 temperature pressure humidity / solution;
repeated / type = AR(1) subject = id; 
run;
```

I'd use the same data structure as above for this outcome as well.

## c. Binary outcome for medication

In order to fit a GzLMM model with a random intercept for subject, I would use PROC GLIMMIX in SAS (pretty much the same code as above, but with distribution = binary). PROC NLMIXED would also work, but I find it a little more confusing. The default in PROC GLIMMIX is to approximate the true likelihood using Laplace's method, but you can specify method = quad to use adaptive Gaussian quadrature. This can also be done using glmer() in R. 

One drawback of quadrature is that it only approximates the true likelihood and there's a bias/variance tradeoff. Also, page 19 of the s13 says that "the GzLMM quadrature approach overestimates SEâ€™s since it does not account for the underdispersion," although there was some debate about this in office hours. Finally, you may end up with a different number of quadtrature points using PROC NLMIXED vs. PROC GLIMMIX. This likely wouldn't make a big difference, but you do have to be a little careful.

## d. Random intercept and serial correlation

Correlated count data like this probably requires a generalized linear mixed model (GzLMM) where the outcome is modeled as Poisson-distributed, a random intercept for subject, and with an AR(1) or spatial power correlation for repeated measures. I think the best way to do this in SAS is to use PROC GLIMMIX, and in R glmmPQL() should work. 

There isn't a REPEATED statement in PROC GLIMMIX, so you need to include another random effect with the "_residual_" keyword and the correlation structure. Something like:

```{sas eval=FALSE}
proc glimmix data=albuterol;
model albuterol_use = 
  friday ln_mmax_pm25 temperature pressure humidity / solution distribution=binary;
random intercept / subject=id;
random _residual_ / subject=id type=ar(1); 
run;
```

The R code would be something like:

```{r eval=FALSE}
glmmPQL(fixed = albuterol_use ~ friday + ln_mmax_pm25 + temperature + 
          pressure + humidity,random = ~1|id, family = "binomial",
        correlation = corAR1(),data = df)
```

Estimation using pseudolikelihoods can be biased, although this goes away with large sample sizes. Also, you cannot specify non-simple R matrices or have random effects at multiple levels.  

\pagebreak

# 2. Albuterol data

## a. GEE

The results of PROC GENMOD are below, with the scale parameter in the red box:

![](/Users/timvigers/Documents/GitHub/School/Analysis of Longitudinal Data/Homework 6/2a.png)

Adding the scale parameter generally increases the standard errors (except for friday = 0 for some reason which I can't figure out).

## b. GzLMM

![](/Users/timvigers/Documents/GitHub/School/Analysis of Longitudinal Data/Homework 6/2b.png)

The residual estimate is equivalent to the scale parameter $\hat{\phi}$, while GEE reports $\sqrt{\hat{\phi}}$. So, to compare the two, $\sqrt{0.7659} = 0.87$ which is pretty close to the scale parameter from the GEE (0.857), although slightly higher. 


## c. Dispersion

These scale parameter estimates suggest underdispersion because they are less than 1.

## d. Slopes and SEs

GzLMM:

![](/Users/timvigers/Documents/GitHub/School/Analysis of Longitudinal Data/Homework 6/2d GzLMM.png)

GEE MBSE:

![](/Users/timvigers/Documents/GitHub/School/Analysis of Longitudinal Data/Homework 6/2d MBSE.png)

GEE Empirical:

![](/Users/timvigers/Documents/GitHub/School/Analysis of Longitudinal Data/Homework 6/2d Emp.png)

The estimate from the GzLMM is slightly higher than from the model-based GEE approach and the SE is slightly lower, but they're very close. The SE from the empirical GEE approach is smaller than both of the others.

## e. Interpretation

The SD of the pollution variable is 0.592, so for each 1 SD increase in pollution, the rate of children's albuterol use increases by about 3.29% (95% CI: 1.09% - 5.55%, p = 0.0033).

```{r include=FALSE}
sd <- sd(df$ln_mmax_pm25,na.rm = T)
est <- 0.05471
exp(est * sd)
ll <- est - 1.96*0.01861
exp(ll*sd)
ul <- est + 1.96*0.01861
exp(ul*sd)
```

# 3. Exacerbation data

## a. Parameter estimates

For each 1 unit increase in day, odds of an exacerbation are 1.009 times higher (95% CI: 1.004 - 1.014, p < 0.0001). Or, for each week odds of an exacerbation are 1.066 times higher (95% CI: 1.034 - 1.099, p < 0.0001) and for each month odds of an exacerbation are 1.316 times higher (95% CI: 1.156 - 1.497, p < 0.0001). 

The odds of an exacerbation are 0.812 times lower (95% CI: 0.695 - 0.948, p = 0.0084) on the weekend compared to weekdays. 

I don't think the two approaches differ much, but it does sort of depend on what counts as a meaningful change in exacerbation odds. The approach with no repeated measures says that odds of exacerbation are 1.01 times higher per increase in day. which is very close to 1.009. However, the approach without AR(1) says that the odds of an exacerbation are 0.794 times lower on the weekend, which is a little lower than 0.812. It seems close enough to me, but I suppose it's possible that the difference is clincally meaningful.

```{r include=FALSE}
day_est <- 0.009147
day_se <- 0.002195
day_ll <- day_est - 1.96*day_se
day_ul <- day_est + 1.96*day_se
weekend_est <- -0.2085
weekend_se <- 0.07911
weekend_ll <- weekend_est - 1.96*weekend_se
weekend_ul <- weekend_est + 1.96*weekend_se
```

## b. SS vs. PA effects

The slope estimates have subject-specific interpretations, because MSPL was used to approximate the likelihood. This option means that instead of averaging the function over subjects, it determines the function for the average subject. The pseudodata is expanded around subjects as opposed to the population.

## c. Estimates

A GzLM/GEE will have population-averaged interpretations, which will generally be lower than the subject-specific beta estimates. The slope of the marginal mean is more attenuated (i.e. a flatter curve) than the conditional mean in a logistic model. This makes some intuitive sense, because if you're averaging the function across multiple subjects, it's impossible for the average to be steeper than the subject specific curves.