---
title: "Longitudinal Homework 3"
author: "Tim Vigers"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(reshape2)
library(lme4)
library(nlme)
library(emmeans)
library(tidyverse)
```

# 1. Cell counts

Starting with the subject-level model, define Z, G, and R matrices:

$$
Z_i = \begin{bmatrix}
    1 \\
    1 \\
    1 \\
    1
\end{bmatrix}
$$

$$
G_i = \sigma^2_0
$$

$$
R_i = \begin{bmatrix}
    1 & \phi & \phi^2 & \phi^3 \\
    \phi & 1 & \phi & \phi^2 \\
    \phi^2 & \phi & 1 & \phi \\
    \phi^3 & \phi^2 & \phi & 1
\end{bmatrix}
$$

$V_i$ is the variance of $Y_i$, so:

$$
Var(Y_i) = Z_iG_iZ_i^t + \sigma^2_\epsilon R_i \\
$$

$$
= \begin{bmatrix}
    \sigma^2_0 & \sigma^2_0 & \sigma^2_0 & \sigma^2_0 \\
    \sigma^2_0 & \sigma^2_0 & \sigma^2_0 & \sigma^2_0 \\
    \sigma^2_0 & \sigma^2_0 & \sigma^2_0 & \sigma^2_0 \\
    \sigma^2_0 & \sigma^2_0 & \sigma^2_0 & \sigma^2_0
\end{bmatrix} +
\begin{bmatrix}
    \sigma^2_\epsilon & \phi\sigma^2_\epsilon & \phi^2\sigma^2_\epsilon & \phi^3\sigma^2_\epsilon \\
    \phi\sigma^2_\epsilon & \sigma^2_\epsilon & \phi\sigma^2_\epsilon & \phi^2\sigma^2_\epsilon \\
    \phi^2\sigma^2_\epsilon & \phi\sigma^2_\epsilon & \sigma^2_\epsilon & \phi\sigma^2_\epsilon \\
    \phi^3\sigma^2_\epsilon & \phi^2\sigma^2_\epsilon & \phi\sigma^2_\epsilon & \sigma^2_\epsilon
\end{bmatrix}\\
$$

$$
=\begin{bmatrix}
    \sigma^2_0+\sigma^2_\epsilon & \sigma^2_0+\phi\sigma^2_\epsilon & \sigma^2_0+\phi^2\sigma^2_\epsilon & \sigma^2_0+\phi^3\sigma^2_\epsilon \\
    \sigma^2_0+\phi\sigma^2_\epsilon & \sigma^2_0+\sigma^2_\epsilon & \sigma^2_0+\phi\sigma^2_\epsilon & \sigma^2_0+\phi^2\sigma^2_\epsilon \\
    \sigma^2_0+\phi^2\sigma^2_\epsilon & \sigma^2_0+\phi\sigma^2_\epsilon & \sigma^2_0+\sigma^2_\epsilon & \sigma^2_0+\phi\sigma^2_\epsilon \\
    \sigma^2_0+\phi^3\sigma^2_\epsilon & \sigma^2_0+\phi^2\sigma^2_\epsilon & \sigma^2_0+\phi\sigma^2_\epsilon & \sigma^2_0+\sigma^2_\epsilon
\end{bmatrix}
$$

Specifying a G and an R matrix gives a more flexible model that accounts for both within-subject correlation and the decaying correlation between time points. If you only used the AR(1) structure, then the variance will go to 0 as the time points get farther apart. When $\sigma^2_0$ is added this doesn't happen. 

# 2. Mt. Kilimanjaro 

$$
G_i = \begin{pmatrix}
\sigma^2_I & \sigma^2_{IS}\\
\sigma^2_{IS} & \sigma^2_S
\end{pmatrix}
$$

$$
Z_i = \begin{pmatrix}
1 & 0\\
1 & 1\\
1 & 2
\end{pmatrix}
$$

$$
Z_i^t = \begin{pmatrix}
1 & 1&1\\
0 & 1&2\\
\end{pmatrix}
$$

$$
R_i = \begin{pmatrix}
\sigma^2_e & 0 & 0\\
0 & \sigma^2_e & 0\\
0 & 0 & \sigma^2_e
\end{pmatrix}
$$

$$
V_i = Var(Y_i) = Z_iG_iZ_i^t + R_i=\begin{pmatrix}
1 & 0\\
1 & 1\\
1 & 2
\end{pmatrix}\begin{pmatrix}
\sigma^2_I & \sigma^2_{IS}\\
\sigma^2_{IS} & \sigma^2_S
\end{pmatrix}\begin{pmatrix}
1 & 1&1\\
0 & 1&2\\
\end{pmatrix}+\begin{pmatrix}
\sigma^2_e & 0 & 0\\
0 & \sigma^2_e & 0\\
0 & 0 & \sigma^2_e
\end{pmatrix}\\
$$

$$
= \begin{pmatrix}
\sigma^2_I+\sigma^2_e & \sigma^2_{IS} + \sigma^2_I & 2\sigma^2_{IS} + \sigma^2_I\\
\sigma^2_{IS} + \sigma^2_I & 2\sigma^2_{IS}+\sigma^2_I+\sigma^2_S+\sigma^2_e & 3\sigma^2_{IS}+\sigma^2_I+2\sigma^2_S\\
2\sigma^2_{IS}+\sigma^2_I & 3\sigma^2_{IS}+\sigma^2_I+2\sigma^2_S & 4\sigma^2_{IS}+\sigma^2_I+4\sigma^2_S+\sigma^2_e
\end{pmatrix}
$$

In order to show this, you compare the covariance for times 0 and 1, and for times 0 and 2. If there's more correlation between time 0 and 1 than there is between time 0 and 2, then cov(0,1) > cov(0,2). Additionally you want cov(0,2) < cov(1,2). This turns out to be fairly easy to rearrange, and shows that there can be decay as time between measurements increases, as long as the below conditions are met:

$$
\sigma^2_{IS}+ \sigma^2_I > 2\sigma^2_{IS} + \sigma^2_I < 3\sigma^2_{IS}+\sigma^2_I+2\sigma^2_S
$$

$$
0 > \sigma^2_{IS}  < 2\sigma^2_{IS}+2\sigma^2_S\\
$$

There must be an inverse relationship between the random effects ($\sigma^2_{IS}<0$) and $\sigma^2_{IS}+2\sigma^2_S$ must be greater than 0.

# 3. Beta carotene data

Convert the data to long form and make a continuous time variable (using baseline 2 as time 0):

```{r import and format data}
# Read in
carotene <- read.csv("/Users/timvigers/Documents/GitHub/School/Analysis of Longitudinal Data/Homework 3/beta_carotene_data.csv")
# Wide to long
long <- melt(carotene,id.vars = c("Id","Prepar"))
# Use baseline 2 as time 0
long <- long %>% filter(variable != "Base1lvl") %>% arrange(Id,variable)
# Continuous time 
long$time <- long$variable
long$time <- plyr::mapvalues(long$time,
          from = c("Base2lvl","Wk6lvl","Wk8lvl","Wk10lvl","Wk12lvl"),
          to = c(0,6,8,10,12))
long$time <- as.numeric(as.character(long$time))
long$Prepar <- as.factor(long$Prepar)
kable(head(long,10))
```

Fit a polynomial model for time and compare AIC and BIC to determine the sufficient degree:

```{r time polynomial}
# Models
# Time polynomials
lin_mod <- gls(value ~ time*Prepar,
            data = long, method = "ML",correlation=corSymm(form = ~1|Id),
            weights = varIdent(form = ~1|time))
quad_mod <- gls(value ~ time*Prepar + 
                    I(time^2)*Prepar, 
            data = long, method = "ML",correlation=corSymm(form = ~1|Id),
            weights = varIdent(form = ~1|time))
cub_mod <- gls(value ~ time*Prepar + 
                    I(time^2)*Prepar + 
                   I(time^3)*Prepar, 
            data = long, method = "ML",correlation=corSymm(form = ~1|Id),
            weights = varIdent(form = ~1|time))
quart_mod <- gls(value ~ time*Prepar + 
                    I(time^2)*Prepar + 
                   I(time^3)*Prepar + 
                     I(time^4)*Prepar, 
            data = long, method = "ML",correlation=corSymm(form = ~1|Id),
            weights = varIdent(form = ~1|time))
kable(AIC(lin_mod,quad_mod,cub_mod,quart_mod))
kable(BIC(lin_mod,quad_mod,cub_mod,quart_mod))
```

The cubic model is slightly lower by AIC and definitely better by BIC, so we'll continue with this model. 

## a. Compare to class variable model

The cubic model can be written:

$$
Y_{hi} = \mu + \alpha_1 + \alpha_2 + \alpha_3 + \tau_h + \gamma_{1h}+ \gamma_{2h}+ \gamma_{3h} + b_i + \epsilon_{hi}
$$

$$
b_i\text{ ~ iid }N(0,\sigma^2_b)
$$

$$
\epsilon_{hi}\text{ ~ iid }N(0,\sigma^2_{\epsilon})\text{ and }\epsilon_{i}\text{ ~ iid }N(0,R_i)\text{ where }R_i\text{ is unstructured}
$$

Here h represents group and i represents subject. The way this model is written, $\alpha_1,\alpha_2,\text{ and } \alpha_3$ represent the effect of time, time squared, and time cubed respectively, and $\tau_h$ is the main effect of group. $\gamma_{1h},\gamma_{2h},\text{ and } \gamma_{3h}$ represent the interaction effects of group and time, time squared, and time cubed respectively. $b_i$ is the random intercept for subject and $\epsilon_{hi}$ is the error term. 

Now compare this to the linear model with group, time and group*time as categorical variables:

```{r class variable}
class_mod <- gls(value ~ factor(variable)*Prepar,
            data = long, method = "ML",correlation=corSymm(form = ~1|Id),
            weights = varIdent(form = ~1|time))
kable(AIC(cub_mod,class_mod))
kable(BIC(cub_mod,class_mod))
```

I think I would include the cubic model in the report, even though I'm not particularly comfortable interpreting polynomial models, and they can be really difficult to explain to investigators. Also, the categorical variable model includes a lot of parameters and the cubic model was slightly better by AIC and much better by BIC. That said, the class model is slightly easier to interpret and not much worse by AIC, so I think there are good reasons to report either one depending on the audience and question. 

## b. Contrast

```{r contrast, warning=FALSE,message=FALSE,cache=TRUE}
# By group, time, and group*time
emm_group <- emmeans(cub_mod, specs = ~Prepar)
emm_group
group1 <- c(1,0,0,0)
group4 <- c(0,0,0,1)
contrast(emm_group, method = list("Group 1 vs. group 4" = group4 - group1))
```

The estimate above compares the group 1 mean to the group 4 mean at the average time (7.2). The difference between the two is not statistically significant (p=0.37).

# 4. Children and schools measured over time

Write out the model:

$$
Y_{hij} = \text{fixed effects} + b_h + b_{i(h)} + \epsilon_{hij} 
$$

$$
b_h\text{ ~ } N(0,\sigma^2_{sch})
$$

$$
b_{i(h)}\text{ ~ }N(0,\sigma^2_{sub}) 
$$

$$
\epsilon\text{ ~ }N(0,\sigma^2_{\epsilon})
$$

Next write out the Z, G and  matrices for a school h:

$$
Z_h = \begin{bmatrix}
    1 & 1 & 0 & 0 \\
    1 & 1 & 0 & 0 \\
    1 & 1 & 0 & 0 \\
    1 & 0 & 1 & 0 \\
    1 & 0 & 1 & 0 \\
    1 & 0 & 1 & 0 \\
    1 & 0 & 0 & 1 \\
    1 & 0 & 0 & 1 \\
\end{bmatrix} 
$$

$$
G_h = \begin{bmatrix}
    \sigma^2_{sch} & 0 & 0 & 0\\
    0 & \sigma^2_{sub} & 0 &0 \\
    0 & 0& \sigma^2_{sub} & 0 \\
    0 & 0 & 0 & \sigma^2_{sub}
\end{bmatrix}
$$

$$
R_h = \sigma^2_\epsilon I_{8x8}
$$

Then use $V_h=Z_hG_hZ_h^t + R_h$:

$$
Z_hG_hZ_h^t = \begin{bmatrix}
    \sigma^2_{sch} & \sigma^2_{sub} & 0 & 0 \\
    \sigma^2_{sch} & \sigma^2_{sub} & 0 & 0 \\
    \sigma^2_{sch} & \sigma^2_{sub} & 0 & 0 \\
    \sigma^2_{sch} & 0 & \sigma^2_{sub} & 0 \\
    \sigma^2_{sch} & 0 & \sigma^2_{sub} & 0 \\
    \sigma^2_{sch} & 0 & \sigma^2_{sub} & 0 \\
    \sigma^2_{sch} & 0 & 0 & \sigma^2_{sub} \\
    \sigma^2_{sch} & 0 & 0 & \sigma^2_{sub} \\
\end{bmatrix} * \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 
\end{bmatrix} \\
$$

$$
Z_hG_hZ_h^t=\begin{bmatrix}
    \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   \\
    \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   \\
    \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   \\
    \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch} & \sigma^2_{sch} \\
    \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch} & \sigma^2_{sch} \\
    \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch} & \sigma^2_{sch} \\
    \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}+\sigma^2_{sub}   & \sigma^2_{sch}+\sigma^2_{sub}   \\
    \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}+\sigma^2_{sub}   & \sigma^2_{sch}+\sigma^2_{sub}   
\end{bmatrix}\\
$$

$$
Z_hG_hZ_h^t +R_h=\begin{bmatrix}
    \sigma^2_{sch}+\sigma^2_{sub}+\sigma^2_\epsilon & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   \\
    \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub}+\sigma^2_\epsilon & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   \\
    \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub}+\sigma^2_\epsilon & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   \\
    \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}+\sigma^2_{sub}+\sigma^2_\epsilon & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch} & \sigma^2_{sch} \\
    \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub}+\sigma^2_\epsilon & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch} & \sigma^2_{sch} \\
    \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub} & \sigma^2_{sch}+\sigma^2_{sub}+\sigma^2_\epsilon & \sigma^2_{sch} & \sigma^2_{sch} \\
    \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}+\sigma^2_{sub} +\sigma^2_\epsilon& \sigma^2_{sch}+\sigma^2_{sub}   \\
    \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}   & \sigma^2_{sch}+\sigma^2_{sub}   & \sigma^2_{sch}+\sigma^2_{sub}+\sigma^2_\epsilon   
\end{bmatrix}
$$