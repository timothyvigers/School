---
title: "Longitudinal Homework 2"
author: "Tim Vigers"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(nlme)
library(MASS)
library(emmeans)
```

# 1. Estimability

If you have a matrix X which contains an intercept and each level of a categorical variable, then this matrix must be less than full rank (LTFR) because a column will be linearly dependent on all the other one. For example, take a matrix for a class variable with three levels:

$$
X_{ltfr} = \begin{bmatrix}
    1 & 0 & 1 & 0 \\
    1 & 0 & 1 & 0 \\
    1 & 1 & 0 & 0 \\
    1 & 0 & 0 & 1
\end{bmatrix}
$$

When a matrix has linear dependencies like this, the specific inverse of X doesn't exist, which means that $\hat{\beta}=(X^tX)^{-1}X^tY$ is not unique. However, if you drop one level of the class variable, you can get a matrix that is full rank:

$$
X_{fr} = \begin{bmatrix}
    1 & 1 & 0 \\
    1 & 1 & 0 \\
    1 & 0 & 0 \\
    1 & 0 & 1
\end{bmatrix}
$$

This matrix no longer has any linearly dependent columns, which means that $X_{fr}^tX_{fr}$ is full rank and the inverse can be computed. When fitting a LTFR model, SAS would use $X_{fr}^tX_{fr}$ to create the generalized inverse (essentially by filling $X_{fr}^tX_{fr}$ in with 0s so that it matches the dimensions of $X_{ltfr}^tX_{ltfr}$). This allows you to estimate the parameters for the class variable, although the beta estimates are not unique since you could use any level of the variable as the reference group.

# 2. $Var(\hat{\beta})$ for full-rank linear mixed model

The algebraic form of $\hat{\beta}$ from ML estimation is:

$$
\hat{\beta} = (X^t V^{-1}X)^{-1}X^t V^{-1}Y
$$

Using the general property $Var(AY) = AVar(Y)A^t$ we can set $A=(X^t V^{-1}X)^{-1}X^t V^{-1}$ and get:

$$
Var(\hat{\beta}) = AVar(Y)A^t=(X^t V^{-1}X)^{-1}X^t V^{-1}Var(Y)((X^t V^{-1}X)^{-1}X^t V^{-1})^t
$$

Because $V^{-1}$ is symmetric, $A^t$ can be re-written as

$$
((X^t V^{-1}X)^{-1}X^t V^{-1})^t = V^{-1}X(X^tV^{-1}X)^{-1}
$$

Leaving us with:

$$
Var(\hat{\beta}) = (X^t V^{-1}X)^{-1}(X^t V^{-1}Var(Y)V^{-1}X)(X^tV^{-1}X)^{-1}
$$

Finally, because $Var(Y)=V$, we get:

$$
Var(\hat{\beta}) = (X^t V^{-1}X)^{-1}(X^t V^{-1}VV^{-1}X)(X^tV^{-1}X)^{-1}\\
= (X^t V^{-1}X)^{-1}(X^t V^{-1}X)(X^tV^{-1}X)^{-1}\\
= (X^t V^{-1}X)^{-1}
$$

# 3. $Var(L\hat{\beta})$

It follows from the above equations that if X does not have full rank, $Var(\tilde\beta)=(X^tV^{-1}X)^-$ (see course notes page 143):

$$
\tilde\beta = (X^t V^{-1}X)^{-}X^t V^{-1}Y\\
Var(\tilde\beta) = (X^t V^{-1}X)^{-}X^t V^{-1}V((X^t V^{-1}X)^{-}X^t V^{-1})^t\\
= (X^t V^{-1}X)^{-}X^t ((X^t V^{-1}X)^{-}X^t V^{-1})^t = (X^t V^{-1}X)^{-}\\
$$

So:

$$
Var(L\tilde\beta) = LVar(\tilde\beta)L^t = L(X^tV^{-1}X)^-L^t
$$

# 4. GLM vs. LMM

The general linear model (GLM) is a special case of a linear mixed model (LMM). Basically, a GLM is a LMM with no random effects and a simple covariance structure ($R=\sigma^2I$). So you can write the GLM as $Y=X\beta+\epsilon$ and the LMM as $Y=X\beta+Zb+\epsilon$, where $b$ is the random effect term and Z is the associated design matrix of the random effects. Including the random effects can account for correlation within clusters (e.g using a random intercept to account for correlations between multiple measures on the same person). Adjusting the correlation structure in a linear model can also allow you to fine-tune the model based on how you believe the correlation between measures is affected by clustering (e.g. using AR(1) to model equally spaced time points).

# 5. Dog data

```{r dog data, echo=FALSE}
# Read in data
dogs <- read.csv("/Users/timvigers/Documents/GitHub/School/Analysis of Longitudinal Data/Homework 2/dog_data.csv")
```

## The model

```{r dog model}
dog_mod <- lme(y ~ group*factor(time),random = ~1|id,data = dogs)
```

## Model results

```{r dog summary, echo=FALSE}
kable(summary(dog_mod)$tTable)
```

## Contrasts with SAS

### Contrast code

![](/Users/timvigers/Documents/GitHub/School/Analysis of Longitudinal Data/Homework 2/Code.png)

### Contrast results

![](/Users/timvigers/Documents/GitHub/School/Analysis of Longitudinal Data/Homework 2/Contrasts.png)

For this problem I assumed that dogs were unique across groups.

First, I compared the ch group to the cl group averaged across time. The estimated difference in gall bladder volume between the two groups is 2.59, but this difference is not statistically significant (p = 0.52). I also compared time 0 to time 120, to see if there was a significant difference in gall bladder size averaged across the groups. The estimated difference is 0.39, but this was also not statistically significant (p = 0.16).

I also contrasted the difference between group ch at time 0 and group co at time 60 to the difference between group ch at time 0 and group cl at time 60. The difference between these differences was not statistically significant (p = 0.45). Similarly, I compared the difference between group ch at time 0 and group co at time 90 to the difference between group ch at time 0 and group cl at time 90. This contrast was also not statistically significant (p = 0.15).

## Confirm with emmeans

```{r contrasts, warning=FALSE, message=FALSE}
emm_group <- emmeans(dog_mod, specs = ~group)
emm_time <- emmeans(dog_mod, specs = ~factor(time))
emm_group_time <- emmeans(dog_mod, specs = ~group*factor(time))
ch <- c(1,0,0)
cl <- c(0,1,0)
t0 <- c(1,0,0,0,0)
t120 <- c(0,0,0,0,1)
contrast(emm_group, method = list("Group ch vs. group cl" = ch - cl))
contrast(emm_time, method = list("Time 0 vs. time 120" = t0 - t120))
```

Unfortunately I didn't have time to figure out the two row contrasts in R but the estimates match, which is a good sign.

# 6.

## a. Model and X matrix

### i. LTFR

The statistical model is:

$$
Y_{ij}= \mu + \alpha_1 + \alpha_2 + \alpha_3 + \tau_1 + \tau_2 + \tau_3 + \gamma_{11} + \gamma_{12} + \gamma_{13} + \gamma_{21} + \gamma_{22} + \gamma_{23} + \gamma_{31} + \gamma_{32} + \gamma_{33} + b_{subject} +\epsilon
$$
Where $\alpha$s represent the effect of time, $\tau$s represent the effect of group, and $\gamma$s represent the interaction effects. $b$ represents a random intercept for subject, and $\epsilon$ is the error term.

The X matrix is:

$$
X = \begin{bmatrix}
    1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
    1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
    1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
    1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
$$

### ii. With a set-to-0 restriction

The statistical model is:

$$
Y_{ij}= \beta_0 + \beta_1x_{group1} + \beta_2x_{group2} + \beta_3x_{time1} + \beta_4x_{time2} + \beta_5x_{group1time1} + \beta_6x_{group1time2} + \beta_7x_{group2time1} + \beta_8x_{group2time2}+ b_{subject} +\epsilon
$$

The X matrix is:

$$
X = \begin{bmatrix}
    1 & 1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    1 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
    1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
    1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
    1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
    1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\
    1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\
    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
$$

This is the same as the matrix in i., but with the highest values of group and time removed.

## b. Linear trend for one group compared to another.

The estimate for group 3, time 1 is:

$$
L_{13}=\begin{bmatrix}
    1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 
\end{bmatrix}
$$

And the estimate for group 1, time 1 is:

$$
L_{11}=\begin{bmatrix}
    1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 
\end{bmatrix}
$$

The linear trend is just the difference between the two:

$$
L_{trend}=\begin{bmatrix}
    0 & 0 & 0 & 0 & -1 & 0 & 1 & -1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 
\end{bmatrix}
$$

Using the LTFR design matrix (since the contrasts above are for the LTFR model), show that $L=LH$:

```{r matrix multiplication}
# Construct the design matrix
x <- matrix(c(1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 
              1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 
              1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 
              1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 
              1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 
              1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 
              1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 
              1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 
              1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 
              1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 
              1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 
              1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 
              1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 
              1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 
              1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
              1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
              1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 
              1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1),
            nrow = 18,byrow = T)
# Construct the hat matrix
H <- ginv((t(x)%*%x))%*%(t(x)%*%x)
# Construct L
L <- c(0,0,0,0,-1,0,1,-1,0,1,0,0,0,0,0,0)
# Test if L = LH (round to 1 digit because the ginv() function produces very 
# small numbers that are essentially 0 but not quite, I think due to 
# machine precision)
round(L %*% H)
round(L %*% H) == L
```

So, the linear trend above is estimable. 

## c. AR(1)

An AR(1) structure for R would allow the correlation between time 1 and time 3 to be lower than the correlation between time 1 and time 2. The distribution of the error term is $\epsilon\text{ ~ }N(0,R_i)$, so this is the major change to the model.

## d. Time as continuous

The statistical model is:

$$
Y_{ij}= \mu + \alpha + \tau_1 + \tau_2 + \tau_3 + \gamma_{1} + \gamma_{2} + \gamma_{3} + b_{subject} +\epsilon
$$

Where $\alpha$ is the effect for time, the $\tau$s are the effects for group, and the $\gamma$s are the interaction effects between time and group.

And the X matrix is:

$$
X = \begin{bmatrix}
    1 & 1 & 1 & 0 & 0 & 1 & 0 & 0 \\
    1 & 1 & 1 & 0 & 0 & 1 & 0 & 0 \\
    1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 \\
    1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 \\
    1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\
    1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\
    1 & 2 & 1 & 0 & 0 & 2 & 0 & 0 \\
    1 & 2 & 1 & 0 & 0 & 2 & 0 & 0 \\
    1 & 2 & 0 & 1 & 0 & 0 & 2 & 0 \\
    1 & 2 & 0 & 1 & 0 & 0 & 2 & 0 \\
    1 & 2 & 0 & 0 & 1 & 0 & 0 & 2 \\
    1 & 2 & 0 & 0 & 1 & 0 & 0 & 2 \\
    1 & 3 & 1 & 0 & 0 & 3 & 0 & 0 \\
    1 & 3 & 1 & 0 & 0 & 3 & 0 & 0 \\
    1 & 3 & 0 & 1 & 0 & 0 & 3 & 0 \\
    1 & 3 & 0 & 1 & 0 & 0 & 3 & 0 \\
    1 & 3 & 0 & 0 & 1 & 0 & 0 & 3 \\
    1 & 3 & 0 & 0 & 1 & 0 & 0 & 3 \\
\end{bmatrix}
$$

## e. Unequally spaced times

### i. Class variable

With unequally spaced time periods, it would not be appropriate to treat time as a categorical variable. Doing so would treat the difference between 0 and 1 month the same as the difference between 0 and 6 months, when in reality the correlation between 0 and 6 months is probably less than the correlation between 0 and 1 month.

### ii. 

In order to account for the unequal spacing of time, you could use a spatial power structure for r:

$$
R_i = \sigma^2 \begin{bmatrix}
    1 & \phi & \phi^5 \\
    \phi & 1 & \phi^3 \\
    \phi^5 & \phi^3 & 1
\end{bmatrix}
$$