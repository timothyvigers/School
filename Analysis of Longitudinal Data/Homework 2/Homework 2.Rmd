---
title: "Longitudinal Homework 2"
author: "Tim Vigers"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(nlme)
library(MASS)
library(emmeans)
```

# 1. Estimability

If you have a matrix X which contains an intercept and each level of a categorical variable, then this matrix must be less than full rank (LTFR) because a column will have to be linearly dependent on another column. For example, take a matrix for a class variable with three levels:

$$
X_{ltfr} = \begin{bmatrix}
    1 & 0 & 1 & 0 \\
    1 & 0 & 1 & 0 \\
    1 & 1 & 0 & 0 \\
    1 & 0 & 0 & 1
\end{bmatrix}
$$

When a matrix has linear dependencies like this, the specific inverse of X doesn't exist, which means that $\hat{\beta}=(X^tX)^{-1}X^tY$ is not unique. However, if you drop one level of the class variable, you can get a matrix with no linear dependencies:

$$
X_{fr} = \begin{bmatrix}
    1 & 1 & 0 \\
    1 & 1 & 0 \\
    1 & 0 & 0 \\
    1 & 0 & 1
\end{bmatrix}
$$

This means that $X_{fr}^tX_{fr}$ is full rank and the specific inverse can be computed. When fitting a LTFR model, SAS would use $X_{fr}^tX_{fr}$ to create the generalized inverse (essentially by filling $X_{fr}^tX_{fr}$ in with 0s so that it matches the dimensions of $X_{ltfr}^tX_{ltfr}$). This allows you to estimate the parameters for the class variable.

# 2. $Var(\hat{\beta})$ for full-rank linear mixed model

The algebraic form of $\hat{\beta}$ from ML estimation is:

$$
\hat{\beta} = (X^t V^{-1}X)^{-1}X^t V^{-1}Y
$$

Using the general property $Var(AY) = AVar(Y)A^t$ we can set $A=(X^t V^{-1}X)^{-1}X^t V^{-1}$ and get:

$$
Var(\hat{\beta}) = AVar(Y)A^t=(X^t V^{-1}X)^{-1}X^t V^{-1}Var(Y)((X^t V^{-1}X)^{-1}X^t V^{-1})^t
$$

Because $V^{-1}$ is symmetric, $A^t$ can be re-written as

$$
((X^t V^{-1}X)^{-1}X^t V^{-1})^t = V^{-1}X(X^tV^{-1}X)^{-1}
$$

Leaving us with:

$$
Var(\hat{\beta}) = (X^t V^{-1}X)^{-1}(X^t V^{-1}Var(Y)V^{-1}X)(X^tV^{-1}X)^{-1}
$$

Finally, because $Var(Y)=V$, we get:

$$
Var(\hat{\beta}) = (X^t V^{-1}X)^{-1}(X^t V^{-1}VV^{-1}X)(X^tV^{-1}X)^{-1}\\
= (X^t V^{-1}X)^{-1}(X^t V^{-1}X)(X^tV^{-1}X)^{-1}\\
= (X^t V^{-1}X)^{-1}
$$

# 3. $Var(L\hat{\beta})$

It follows from the above equations that if X does not have full rank, $Var(\tilde\beta)=(X^tV^{-1}X)^-$ (see course notes page 143):

$$
\tilde\beta = (X^t V^{-1}X)^{-}X^t V^{-1}Y\\
Var(\tilde\beta) = (X^t V^{-1}X)^{-}X^t V^{-1}V((X^t V^{-1}X)^{-}X^t V^{-1})^t
$$

Using the M-P general inverse allows you to simplify and get:

$$
(X^t V^{-1}X)^{-}X^t ((X^t V^{-1}X)^{-}X^t V^{-1})^t = (X^t V^{-1}X)^{-}\\
$$

Because $(X^tV^{-1}X)^-X^t$ is unique in a LMM, any generalized inverse will give you the same result.

So:

$$
Var(L\tilde\beta) = LVar(\tilde\beta)L^t = L(X^tV^{-1}X)^-L^t
$$

# 4. GLM vs. LMM

The general linear model (GLM) is a special case of a linear mixed model (LMM). Basically, a GLM is a LMM with no random effects and a simple covariance structure ($R=\sigma^2I$). So you can write the GLM as $Y=X\beta+\epsilon$ and the LMM as $Y=X\beta+Zb+\epsilon$, where $b$ is the random effect term and Z is the associated design matrix of the random effects. Including the random effects can account for correlation within clusters (e.g using a random intercept to account for correlations between multiple measures on the same person). Adjusting the correlation structure in a linear model can also allow you to fine-tune the model based on how you believe the correlation between measures is affected by clustering (e.g. using AR(1) to model equally spaced time points).

# 5. Dog data

```{r dog data, echo=FALSE}
# Read in data
dogs <- read.csv("/Users/timvigers/Documents/GitHub/School/Analysis of Longitudinal Data/Homework 2/dog_data.csv")
```

## The model

```{r dog model}
dog_mod <- lme(y ~ group*factor(time),random = ~1|id,data = dogs)
```

## Model results

```{r dog summary, echo=FALSE}
kable(summary(dog_mod)$tTable)
```

## Contrasts with SAS

### Contrast code

![](/Users/timvigers/Documents/GitHub/School/Analysis of Longitudinal Data/Homework 2/Code.png)

### Contrast results

![](/Users/timvigers/Documents/GitHub/School/Analysis of Longitudinal Data/Homework 2/Contrasts.png)

For this problem I assumed that dogs were unique across groups.

First, I compared the ch group to the cl group averaged across time. The estimated difference in gall bladder volume between the two groups is 2.59, but this difference is not statistically significant (p = 0.52). I also compared time 0 to time 120, to see if there was a significant difference in gall bladder size averaged across the groups. The estimated difference is 0.39, but this was also not statistically significant (p = 0.16).

I also contrasted the difference from time 0 to 60 for the ch group to the difference between 0 and 60 for the co group. The two trends were significantly different (p = <0.0001). Similarly, I compared the difference from time 0 to 90 for the ch group to the difference between 0 and 90 for the co group. This contrast was also statistically significant (p = 0.0002).

## Confirm with emmeans

```{r contrasts, warning=FALSE, message=FALSE}
emm_group <- emmeans(dog_mod, specs = ~group)
emm_time <- emmeans(dog_mod, specs = ~factor(time))
emm_group_time <- emmeans(dog_mod, specs = ~group*factor(time))
ch <- c(1,0,0)
cl <- c(0,1,0)
t0 <- c(1,0,0,0,0)
t120 <- c(0,0,0,0,1)
contrast(emm_group, method = list("Group ch vs. group cl" = ch - cl))
contrast(emm_time, method = list("Time 0 vs. time 120" = t0 - t120))
```

Unfortunately I didn't have time to figure out the double row contrasts in R but the ones I was able to do match the SAS estimates, which is a good sign.

# 6.

## a. Model and X matrix

### i. LTFR

The statistical model is:

$$
Y_{hij}= \mu + \alpha_j + \tau_h + \gamma_{hj} + b_{i} +\epsilon_{hij}
$$
Where i indexes subject, j indexes time, and h indexes group. Also, $\alpha$ represents the effect of time, $\tau$ represents the effect of group, $\gamma$ represents the interaction effect, $b$ represents a random intercept for subject, and $\epsilon$ is the error term.

The X matrix is:

$$
X = \begin{bmatrix}
    1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
    1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
    1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
    1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
$$

### ii. With a set-to-0 restriction

The statistical model is:

$$
Y_{hij}= \beta_0 + \beta_1x_{1ij} + \beta_2x_{2ij} + \beta_3x_{3ih} + \beta_4x_{4ih} + \beta_5x_{5hij} + \beta_6x_{6hij} + \beta_7x_{7hij} + \beta_8x_{8hij}+ b_{i} +\epsilon_{hij}
$$

Again, i indexes subject, j indexes time, and h indexes group. In this model $x_{pij}$ is a dummy variable indicating whether or not a subject is in a given group at a given time.

The X matrix is:

$$
X = \begin{bmatrix}
    1 & 1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    1 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
    1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
    1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
    1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
    1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\
    1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\
    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
$$

This is the same as the matrix in i, but with the highest values of group and time removed.

## b. Linear trend for one group compared to another.

The estimate for group 3, time 1 is:

$$
L_{13}=\begin{bmatrix}
    1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 
\end{bmatrix}
$$

And the estimate for group 1, time 1 is:

$$
L_{11}=\begin{bmatrix}
    1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 
\end{bmatrix}
$$

The linear trend is just the difference between the two:

$$
L_{trend}=\begin{bmatrix}
    0 & 0 & 0 & 0 & -1 & 0 & 1 & -1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 
\end{bmatrix}
$$

Using the LTFR design matrix (since the contrasts above are for the LTFR model), show that $L=LH$:

```{r matrix multiplication}
# Construct the design matrix
x <- matrix(c(1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 
              1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 
              1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 
              1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 
              1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 
              1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 
              1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 
              1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 
              1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 
              1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 
              1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 
              1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 
              1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 
              1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 
              1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
              1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
              1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 
              1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1),
            nrow = 18,byrow = T)
# Construct the hat matrix
H <- ginv((t(x)%*%x))%*%(t(x)%*%x)
# Construct L
L <- c(0,0,0,0,-1,0,1,-1,0,1,0,0,0,0,0,0)
# Test if L = LH (round to 1 digit because the ginv() function produces very 
# small numbers that are essentially 0 but not quite, I think due to 
# machine precision)
round(L %*% H)
round(L %*% H) == L
```

So, the linear trend above is estimable because L=LH. 

## c. AR(1)

An AR(1) structure for R would allow the correlation between time 1 and time 3 to be lower than the correlation between time 1 and time 2. The distribution of the error term is $\epsilon\text{ ~ }N(0,R_i)$, so the major change to the model would be the form of $R_i$ which alters the distribution of the errors.

## d. Time as continuous

The statistical model is:

$$
Y_{hij}= \mu + \alpha + \tau_h + \gamma_{h} + b_{i} +\epsilon_{hij}
$$

Where $\alpha$ is the effect for time, $\tau$ is the effect for group, and $\gamma$ is the interaction effects between time and group.

And the X matrix is:

$$
X = \begin{bmatrix}
    1 & 1 & 1 & 0 & 0 & 1 & 0 & 0 \\
    1 & 1 & 1 & 0 & 0 & 1 & 0 & 0 \\
    1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 \\
    1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 \\
    1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\
    1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\
    1 & 2 & 1 & 0 & 0 & 2 & 0 & 0 \\
    1 & 2 & 1 & 0 & 0 & 2 & 0 & 0 \\
    1 & 2 & 0 & 1 & 0 & 0 & 2 & 0 \\
    1 & 2 & 0 & 1 & 0 & 0 & 2 & 0 \\
    1 & 2 & 0 & 0 & 1 & 0 & 0 & 2 \\
    1 & 2 & 0 & 0 & 1 & 0 & 0 & 2 \\
    1 & 3 & 1 & 0 & 0 & 3 & 0 & 0 \\
    1 & 3 & 1 & 0 & 0 & 3 & 0 & 0 \\
    1 & 3 & 0 & 1 & 0 & 0 & 3 & 0 \\
    1 & 3 & 0 & 1 & 0 & 0 & 3 & 0 \\
    1 & 3 & 0 & 0 & 1 & 0 & 0 & 3 \\
    1 & 3 & 0 & 0 & 1 & 0 & 0 & 3 \\
\end{bmatrix}
$$

## e. Unequally spaced times

### i. Class variable

With unequally spaced time periods, it would be appropriate to treat time as a categorical variable, as long as you were only interested in the difference between each time point and baseline.

### ii. 

In order to account for the unequal spacing of time, you could use a spatial power structure for $R_i$:

$$
R_i = \sigma^2 \begin{bmatrix}
    1 & \phi & \phi^6 \\
    \phi & 1 & \phi^5 \\
    \phi^6 & \phi^5 & 1
\end{bmatrix}
$$