---
title: "BIOS 7659 Homework 6"
author: "Tim Vigers"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE,dpi = 600)
knitr::opts_knit$set(root.dir="C:/Users/tim/Documents/GitHub/School/Statistical Genomics")
library(cqn)
library(edgeR)
library(biomaRt)
library(RUVSeq)
library(blandr)
library(RColorBrewer)
library(tidyverse)
library(flextable)
```

```{r}
data("montgomery.subset")
data("uCovar")
```

# 1. Differential Expression

## a) Calculate RPKM and perform a t test

RPKM stands for reads per kilobases per million reads and is calculated by $$\frac{r_g}{l_g T}=\frac{\text{# of mapped reads}}{\text{length in Kb}*\text{total mapped reads in millions}}:$$

```{r}
lg = uCovar$length / 1000
t = sum(unlist(montgomery.subset)) / 1e6
rpkm = montgomery.subset/(lg*t)
```

Do a standard t test between the groups for each gene (using RPKM):

```{r}
# T test for each row
t_tests = apply(rpkm,1,function(x){
  group1 = as.numeric(x[1:5])
  group2 = as.numeric(x[6:10])
  if(var(group1)==0 | var(group2)==0){ # Skip those with constant values 
    return(c(NA,NA))
  } else {
    t = t.test(group1,group2)
    return(c(t$statistic,t$p.value))
  }
})
# Format and print
t_tests = as.data.frame(t(t_tests))
t_tests = t_tests %>% rownames_to_column() %>% 
  set_names(c("Gene","T","p")) %>% arrange(desc(abs(T))) 

t_tests %>% head(10) %>% flextable(.) %>% 
  set_caption("Top 10 Genes by t-statistic") %>% autofit(.)
```

## b) Plot the histogram of p-values

```{r}
hist(t_tests$p,main = "Histogram of p values",xlab = "p")
```

Normally we would expect a uniform distribution of p values, but this distribution appears to have a peak at around 0.3 and another at p = 1. The slight peak around 0.3 isn't necessarily too concerning, but it is odd that there are so many p values equal to (or very close to) 1. Looking at the genes that produce these p values, it seems as if they all have low RPKM across all samples, but were not excluded in the initial filtering step based on raw counts. Because the values are small, the t test for the difference between the two groups produces a tiny t statistic, which corresponds to a p value of 1. I think it would be best to just exclude these gene from any RPKM-based analyses.

```{r}
p1 = t_tests$Gene[which(t_tests$p==1)]
p1 = rpkm[p1,] %>% rownames_to_column(var="Gene")
flextable(head(p1,10))
```

## c) Filter genes by total counts

```{r}
# Remove genes with low counts
filtered = montgomery.subset[rowSums(montgomery.subset)>=10,]
# Create edgeR object
filtered_dge = DGEList(filtered,group = rep(c(1,2),each=5))
```

There are a total of `r nrow(filtered)` genes with at least 10 counts across all samples. By default, the `DGEList()` function uses column sums for the `lib.size` argument. This makes the most sense for this dataset, as the provided `sizeFactors.subset` data appears to be incorrect.

## d) Calculate TMM normalization factors

```{r}
norm = calcNormFactors(filtered_dge)
autofit(flextable(norm$samples))
```

The effective library sizes are generally similar to the column sums because the normalization factors are fairly close to 1. A normalization factor > 1 increases the library size, which is similar to downscaling the counts (and vice versa for factors < 1). So, the effective library size for sample 6 is increased by about 115%. Conversely, the effective library size for sample 5 is decreased by approximately 90%, which suggests that there are a small number of high-count sequences that need to be adjusted for.

## e) Use the `estimateDisp()` function to calculate the common, trended and tagwise dispersions

```{r}
filtered_dge = estimateDisp(filtered_dge)
```

The common dispersion estimate is approximately `r round(filtered_dge$common.dispersion,3)`. Plot the tagwise dispersion estimate for each gene vs. the average log counts per million:

```{r}
plotBCV(filtered_dge)
```

The common dispersion estimate only seems to work well for a narrow range of log CPM around 4. It appears to underestimate dispersion for the lower count genes and overestimate the higher average count genes. Note that the y axis of the above plot is the biological CV, which is the square root of the dispersion factor. 

## f) Fit the negative binomial model

### Using the common dispersion estimate

```{r}
et = exactTest(filtered_dge,dispersion = "common")
top_common = topTags(et) %>% as.data.frame(.) %>% 
  rownames_to_column(.,var = "Gene")
autofit(flextable(top_common))
```

### Using the tag-wise dispersion estimates

```{r}
et = exactTest(filtered_dge,dispersion = "tagwise")
top_tagwise = topTags(et) %>% as.data.frame(.) %>% 
  rownames_to_column(.,var = "Gene")
autofit(flextable(top_tagwise))
```

There are only `r length(intersect(top_tagwise$Gene,top_common$Gene))` overlapping genes in the top 10 table for the two methods.Also, the top genes as determined by the common dispersion estimate approach appear to be driven more by fold change than those that are most significant using the tagwise method, because the table is essentially in decreasing order of logFC (with a couple of minor exceptions). This is because the common dispersion factor fails to correctly account for dispersion, which means that the fold change drives the significance test.

## g) Extract the raw counts 

### For the top 10 genes based on the common dispersion

```{r}
top_common_counts = filtered_dge$counts[top_common$Gene,] %>%
  as.data.frame(.) %>% rownames_to_column(var = "Gene")
set_table_properties(flextable(top_common_counts),
                     width = .5, layout = "autofit")
```

### For the top 10 genes based on tagwise dispersion

```{r}
top_tagwise_counts = filtered_dge$counts[top_tagwise$Gene,] %>%
  as.data.frame(.) %>% rownames_to_column(var = "Gene")
set_table_properties(flextable(top_tagwise_counts),
                     width = .5, layout = "autofit")
```

The genes that are significant when using the common dispersion tend to be low counts with a few highly expressed genes, whereas the top genes based on tagwise dispersion tend to have counts more evenly spread across the samples. This further supports my theory from above.

### Get Ensembl information for the top genes

#### Based on common dispersion

```{r}
ensembl = useEnsembl(biomart="ensembl", dataset="hsapiens_gene_ensembl")
en = getBM(attributes=c('ensembl_gene_id','description'), 
           filters ='ensembl_gene_id', values = top_common_counts$Gene, 
           mart = ensembl)
set_table_properties(flextable(en),
                     width = .75, layout = "autofit")
```

#### Based on tagwise dispersion

```{r}
en = getBM(attributes=c('ensembl_gene_id','description'), 
           filters ='ensembl_gene_id', values = top_tagwise_counts$Gene, 
           mart = ensembl)
set_table_properties(flextable(en),
                     width = .75, layout = "autofit")
```

Almost all of the top ten genes based on common dispersion code for some sort of immunoglobulin. A few of the top tagwise genes are also immunoglobulins, but there appears to be a greater variety of function in among the top tagwise genes.

# 2. Remove Unwanted Variation.

## a) Create a design matrix 

```{r}
# Fit the GLM 
group = rep(c(1,2),each=5)
design = model.matrix(~group)
filtered_dge = DGEList(filtered)
filtered_dge = calcNormFactors(filtered_dge,"upperquartile")
filtered_dge = estimateDisp(filtered_dge,design = design)
glm_fit_a = glmFit(filtered_dge)
glm_lrt_a = glmLRT(glm_fit_a)$table
glm_lrt_a$PValue_FDR = p.adjust(glm_lrt_a$PValue,"fdr")
```

Using the likelihood ratio test, there are `r sum(glm_lrt_a$PValue_FDR<0.05)` genes with FDR-adjusted p values < 0.05.

## b) Fit edgeR models that adjust for unwanted variation

```{r}
colors = brewer.pal(3, "Set2")
set = newSeqExpressionSet(
  as.matrix(filtered),
  phenoData = data.frame(group,row.names=colnames(filtered)))
set = betweenLaneNormalization(set, which="upper")
plotRLE(set, outline=FALSE,col=colors[group])
plotPCA(set,col=colors[group])
```

Two of the samples in group 2 (NA11918 and NA12006) appear to be very different from the rest of the samples. In the boxplot these samples have a much wider range (more variability) than the others. Also, in the PCA plot there is one cluster with samples from both groups, and the two outliers from group 2 are clearly different from everything else. The first PC appears to be driven by the difference between NA12006 and the other samples, whereas PC2 seems to be driven by NA11918. Between-lane normalization does not appear to be sufficient for these data.

## c) Perform RUVg using negative empirical control genes

```{r}
# take the 10,000 genes with the largest likelihood ratio test 
# p-values from part a)
neg_controls = 
  rownames(head(glm_lrt_a[order(glm_lrt_a$PValue,
                                decreasing = T),],10000))
set1 = RUVg(set,neg_controls,k=1)
plotRLE(set1, outline=FALSE,col=colors[group])
plotPCA(set1,col=colors[group])
```

These plots are definitely an improvement on the previous ones. There still appear to be two samples that are different from the others (NA07037 and NA11918), but in both the boxplot and PCA plot all of the samples are closer to one another than before RUVg. 

```{r}
design = model.matrix(~group + W_1, data=pData(set1))
y = DGEList(counts=counts(set1), group=group)
y = calcNormFactors(y, method="upperquartile")
y = estimateGLMCommonDisp(y, design)
y = estimateGLMTagwiseDisp(y, design)
glm_fit_c = glmFit(y, design)
glm_lrt_c = glmLRT(glm_fit_c, coef=2)$table
glm_lrt_c$PValue_FDR = p.adjust(glm_lrt_c$PValue,"fdr")
```

After controlling for unwanted variation, there are `r sum(glm_lrt_c$PValue_FDR<0.05)` genes with FDR adjusted p values < 0.05.

## d) Repeat part c) using k=2

```{r}
set2 = RUVg(set,neg_controls,k=2)
plotRLE(set2, outline=FALSE,col=colors[group])
plotPCA(set2,col=colors[group])
```

Increasing the number of factors of unwanted variation appears to improve these plots somewhat, although now sample NA07037 looks like a bit of an outlier (particularly in the boxplot). Overall though, the distribution of the samples in the boxplot appears to be more similar than in previous steps, and separation in the PCA plot doesn't appear to be driven as much by single samples. 

```{r}
design = model.matrix(~group + W_1 + W_2, data=pData(set2))
y = DGEList(counts=counts(set2), group=group)
y = calcNormFactors(y, method="upperquartile")
y = estimateGLMCommonDisp(y, design)
y = estimateGLMTagwiseDisp(y, design)
glm_fit_d = glmFit(y, design)
glm_lrt_d = glmLRT(glm_fit_d, coef=2)$table
glm_lrt_d$PValue_FDR = p.adjust(glm_lrt_d$PValue,"fdr")
```

After controlling for two factors of unwanted variation, there are `r sum(glm_lrt_d$PValue_FDR<0.05)` genes with FDR adjusted p values < 0.05.

## e) Repeat part (d) using the RUVr method with k=2

```{r}
# GLM residuals
res = residuals(glm_fit_a, type="deviance")
# RUVr
set3 = RUVr(set,neg_controls,k=2,res)
plotRLE(set3,outline=FALSE,col=colors[group])
plotPCA(set3,col=colors[group])
```

Overall these plots look similar to using RUVg with k = 2, although it appears that NA11918 is still different from the other samples. NA11918 also seems to driving PC1, although this PCA plot is still a significant improvement over the original plot without any RUV methods applied.

```{r}
design = model.matrix(~group + W_1 + W_2, data=pData(set3))
y = DGEList(counts=counts(set3), group=group)
y = calcNormFactors(y, method="upperquartile")
y = estimateGLMCommonDisp(y, design)
y = estimateGLMTagwiseDisp(y, design)
glm_fit_e = glmFit(y, design)
glm_lrt_e = glmLRT(glm_fit_e, coef=2)$table
glm_lrt_e$PValue_FDR = p.adjust(glm_lrt_e$PValue,"fdr")
```

After controlling for two factors of unwanted variation using RUVr, there are `r sum(glm_lrt_e$PValue_FDR<0.05)` genes with FDR adjusted p values < 0.05.

## f) Concerns

Based on the diagnostic plots alone, there doesn't appear to be anything wrong with using RUV on this dataset. However, one of the important assumptions of RUVg is that there are "control" genes that are not differentially expressed. Here we chose 10,000 genes with large p values from a standard likelihood ratio test, which seems like a somewhat ad-hoc approach. It would be better if we could use prior knowledge or spike-ins to determine control genes.

The assumptions for RUVr are less strict, so I would be more inclined to move forward with that method. RUVr can use information from all the genes for normalization, but assumes that the unwanted factors are unrelated to variables of interest. This seems like a reasonable assumption for this dataset, although it's difficult to say for sure.

The major concern I have with RUV in general is that it's easy to accidentally remove wanted variation. This isn't unique to RUV, and I think it can be avoided by carefully checking the diagnostic plots and keeping the k parameter low, but it's a concern nonetheless. 

# 3. Method Comparisons

Load package and data:

```{r}
library(DESeq2)
load(url("http://bowtie-bio.sourceforge.net/recount/ExpressionSets/
bottomly_eset.RData"))
bottomly.count.table = exprs(bottomly.eset)
```

## a) Create a new data frame with genes that have at least 10 counts

```{r}
# Filter
filtered = bottomly.count.table[rowSums(bottomly.count.table)>=10,]
# edgeR object
filtered_dge = DGEList(filtered)
# DESeq2 object
pheno = factor(gsub("/","_",phenoData(bottomly.eset)$strain))
filtered_dseq = DESeqDataSetFromMatrix(filtered, DataFrame(pheno),~pheno)
```

There are `r nrow(filtered)` genes with at least 10 counts across all samples.

## b) Calculate the DESeq2 size factors

```{r}
filtered_dseq = estimateSizeFactors(filtered_dseq)
filtered_dge = calcNormFactors(filtered_dge,method = "TMM")
# Compare
sizes = data.frame(sizeFactors(filtered_dseq))
sizes = cbind(sizes,filtered_dge$samples$norm.factors) %>%
  rownames_to_column() 
colnames(sizes) = c("Sample","DESeq Size Factor","TMM Norm Factor")
autofit(flextable(sizes))
```

Size factors as calculated by DESeq2 are the median of the ratios of each sample over a psuedosample (the same as the RLE method in edgeR). The pseudosample is the geometric mean for each gene across all samples. The formula for the size factor of sample $s_j$ (with $i$ indexing gene) is:

$$
\hat{s_j} = \text{median}_i(\frac{k_{ij}}{(\prod_{v=1}^{k_{iv}})^{\frac{1}{m}}})
$$

The TMM approach estimates relative RNA expression using the trimmed mean of the M values, where the M value of gene g in samples k and k' is:

$$
M_g=\text{log}_2\frac{Y_{gk}/N_k}{Y_{gk'}/N_{k'}}
$$

And $Y$ indicates the observed count.

The general idea of size factors is to make samples which may have been sequenced at different depths more comparable. For this dataset, the size factors estimated by TMM tend to be close to 1, whereas those estimated by DESeq2 have a much larger range and are more variable. 

## c) Calculate the DESeq2 dispersions

Histograms:

```{r}
filtered_dseq = estimateDispersions(filtered_dseq,fitType = "local")
hist(dispersions(filtered_dseq),xlab = "Size Factor",
     main = "Histogram of DESeq2 Size Factors")
filtered_dge = estimateDisp(filtered_dge)
hist(filtered_dge$tagwise.dispersion,xlab = "Dispersion",
     main = "Histogram of edgeR Tagwise Dispersion")
```

Bland-Altman plot:

```{r message=FALSE}
blandr.draw(log(dispersions(filtered_dseq)),
            log(filtered_dge$tagwise.dispersion))
```

## d) Test for differences between the two strains

```{r}
filtered_dseq = nbinomWaldTest(filtered_dseq)
res = results(filtered_dseq)
design = model.matrix(~pheno)
glm_fit = glmFit(filtered_dge,design)
glm_lrt = glmLRT(glm_fit)$table
glm_lrt$PValue_BH = p.adjust(glm_lrt$PValue,"BH")
```

The DESeq2 method finds `r sum(res$padj < 0.05,na.rm = T)` genes with p values < 0.05 after Benjamini-Hochberg (BH) correction, and edgeR finds `r sum(glm_lrt$PValue_BH<0.05)`. 
