---
title: "BIOS 7659 Homework 6"
author: "Tim Vigers"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE,dpi = 600)
knitr::opts_knit$set(root.dir="C:/Users/tim/Documents/GitHub/School/Statistical Genomics")
library(cqn)
library(edgeR)
library(biomaRt)
library(RUVSeq)
library(blandr)
library(RColorBrewer)
library(tidyverse)
library(flextable)
```

```{r}
data("montgomery.subset")
data("uCovar")
```

# 1. Differential Expression

## a) Calculate RPKM and perform a t test

RPKM stands for reads per kilobases per million reads and is calculated by $$\frac{r_g}{l_g T}=\frac{\text{# of mapped reads}}{\text{length in Kb}*\text{total mapped reads in millions}}:$$

```{r}
lg = uCovar$length / 1000
t = sum(unlist(montgomery.subset))
rpkm = montgomery.subset/(lg*t)
```

Do a standard t test between the groups for each gene:

```{r}
# T test for each row
t_tests = apply(rpkm,1,function(x){
  group1 = as.numeric(x[1:5])
  group2 = as.numeric(x[6:10])
  if(var(group1)==0 | var(group2)==0){ # Skip those with constant values 
    return(c(NA,NA))
  } else {
    t = t.test(group1,group2)
    return(c(t$statistic,t$p.value))
  }
})
# Format and print
t_tests = as.data.frame(t(t_tests))
t_tests = t_tests %>% rownames_to_column() %>% 
  set_names(c("Gene","T","p")) %>% arrange(desc(abs(T))) 

t_tests %>% head(10) %>% flextable(.) %>% 
  set_caption("Top 10 Genes by t-statistic") %>% autofit(.)
```

## b) Plot the histogram of p-values

# THIS IS PROBABLY SOMETHING TO DO WITH RPKM - LOOK INTO IT

```{r}
hist(t_tests$p,main = "Histogram of p values",xlab = "p")
```

Normally we would expect a uniform distribution of p values, but this distribution appears to have a peak at around 0.3 and another at p = 1. My guess is that this is because we have only filtered genes with all 0 counts, but kept other genes with counts so low that they are effectively 0. 

## c) Filter genes by total counts

# DOESN'T DGELIST AUTOMATICALLY INCLUDE lib.size
with the total reads per subject?

```{r}
# Remove genes with low counts
filtered = montgomery.subset[rowSums(montgomery.subset)>=10,]
# Create edgeR object
filtered_dge = DGEList(filtered,group = rep(c(1,2),each=5))
```

## d) Calculate TMM normalization factors

```{r}
norm = calcNormFactors(filtered_dge)
autofit(flextable(norm$samples))
```

The effective library sizes are generally similar to the column sums because the normalization factors are fairly close to 1. A normalization factor > 1 increases the library size, which is similar to downscaling the counts (and vice versa for factors < 1). So, the effective library size for sample 6 is increased by about 115%. Conversely, the effective library size for sample 5 is decreased by approximately 90%, which suggests that there are a small number of high-count sequences that need to be adjusted for.

## e) Use the `estimateDisp()` function to calculate the common, trended and tagwise dispersions

```{r}
filtered_dge = estimateDisp(filtered_dge)
```

The common dispersion estimate is approximately `r round(filtered_dge$common.dispersion,3)`. Plot the tagwise dispersion estimate for each gene vs. the average log counts per million:

# IS IT OKAY TO USE THE BUILT IN FUNCTION THAT PLOTS ON SQUARE ROOT SCALE?

```{r}
plotBCV(filtered_dge)
```

The common dispersion estimate only seems to work well for a narrow range of log CPM around 4. It appears to underestimate dispersion for the lower count genes and overestimate the higher average count genes.

## f) Fit the negative binomial model

### Using the common dispersion estimate

```{r}
et = exactTest(filtered_dge,dispersion = "common")
top_common = topTags(et) %>% as.data.frame(.) %>% 
  rownames_to_column(.,var = "Gene")
autofit(flextable(top_common))
```

### Using the tag-wise dispersion estimates

```{r}
et = exactTest(filtered_dge,dispersion = "tagwise")
top_tagwise = topTags(et) %>% as.data.frame(.) %>% 
  rownames_to_column(.,var = "Gene")
autofit(flextable(top_tagwise))
```

There are only `r length(intersect(top_tagwise$Gene,top_common$Gene))` overlapping genes in the top 10 table for the two methods.Also, the top genes as determined by the common dispersion estimate approach appear to be driven more by fold change than those that are most significant using the tagwise method, because the table is essentially in decreasing order of logFC (with a couple of minor exceptions).

# ESSENTIALLY BECAUSE THE VARIABILITY (DISPERSION) ISN'T REALLY ACCOUNTED FOR CORRECTLY WITH COMMON

## g) Extract the raw counts 

### For the top 10 genes based on the common dispersion

```{r}
top_common_counts = filtered_dge$counts[top_common$Gene,] %>%
  as.data.frame(.) %>% rownames_to_column(var = "Gene")
set_table_properties(flextable(top_common_counts),
                     width = .5, layout = "autofit")
```

# Most of these top genes have...

### For the top 10 genes based on tagwise dispersion

```{r}
top_tagwise_counts = filtered_dge$counts[top_tagwise$Gene,] %>%
  as.data.frame(.) %>% rownames_to_column(var = "Gene")
set_table_properties(flextable(top_tagwise_counts),
                     width = .5, layout = "autofit")
```

# These are more evenly spread...

### Get Ensembl information for the top genes

#### Based on common dispersion

```{r}
ensembl = useEnsembl(biomart="ensembl", dataset="hsapiens_gene_ensembl")
en = getBM(attributes=c('ensembl_gene_id','description'), 
           filters ='ensembl_gene_id', values = top_common_counts$Gene, 
           mart = ensembl)
set_table_properties(flextable(en),
                     width = .75, layout = "autofit")
```

#### Based on tagwise dispersion

```{r}
en = getBM(attributes=c('ensembl_gene_id','description'), 
           filters ='ensembl_gene_id', values = top_tagwise_counts$Gene, 
           mart = ensembl)
set_table_properties(flextable(en),
                     width = .75, layout = "autofit")
```

# 2. Remove Unwanted Variation.

## a) Create a design matrix 

```{r}
# Fit the GLM 
group = rep(c(1,2),each=5)
design = model.matrix(~group)
filtered_dge = DGEList(filtered)
filtered_dge = calcNormFactors(filtered_dge,"upperquartile")
filtered_dge = estimateDisp(filtered_dge,design = design)
glm_fit_a = glmFit(filtered_dge,
                 dispersion = filtered_dge$common.dispersion)
glm_lrt_a = glmLRT(glm_fit_a)$table
glm_lrt_a$PValue_FDR = p.adjust(glm_lrt_a$PValue,"fdr")
```

Using the likelihood ratio test, there are `r sum(glm_lrt_a$PValue_FDR<0.05)` genes with FDR-adjusted p values < 0.05.

## b) Fit edgeR models that adjust for unwanted variation

```{r}
colors = brewer.pal(3, "Set2")
set = newSeqExpressionSet(
  as.matrix(filtered),
  phenoData = data.frame(group,row.names=colnames(filtered)))
set = betweenLaneNormalization(set, which="upper")
plotRLE(set, outline=FALSE,col=colors[group])
plotPCA(set,col=colors[group])
```

Two of the samples in group 2 (NA11918 and NA12006) appear to be very different from the rest of the samples. In the boxplot these samples have a much wider range (more variability) than the others. Also, in the PCA plot there is one cluster with samples from both groups, and the two outliers from group 2 are clearly different from everything else. The first PC appears to be driven by the difference between NA12006 and the other samples, whereas PC2 seems to be driven by NA11918. Between-lane normalization does not appear to be sufficient for these data.

## c) Perform RUVg using negative empirical control genes

```{r}
# take the 10,000 genes with the largest likelihood ratio test 
# p-values from part a)
neg_controls = 
  rownames(head(glm_lrt_a[order(glm_lrt_a$PValue,
                                decreasing = T),],10000))
set1 = RUVg(set,neg_controls,k=1)
plotRLE(set1, outline=FALSE,col=colors[group])
plotPCA(set1,col=colors[group])
```

These plots are definitely an improvement on the previous ones. There still appear to be two samples with more variability than the others (NA07037 and NA11918), but in both the boxplot and PCA plot all of the samples are closer to one another than before RUVg. 

```{r}
design = model.matrix(~group + W_1, data=pData(set1))
y = DGEList(counts=counts(set1), group=group)
y = calcNormFactors(y, method="upperquartile")
y = estimateGLMCommonDisp(y, design)
y = estimateGLMTagwiseDisp(y, design)
glm_fit_c = glmFit(y, design)
glm_lrt_c = glmLRT(glm_fit_c, coef=2)$table
glm_lrt_c$PValue_FDR = p.adjust(glm_lrt_c$PValue,"fdr")
```

After controlling for unwanted variation, there are `r sum(glm_lrt_c$PValue_FDR<0.05)` genes with FDR adjusted p values < 0.05.

## d) Repeat part c) using k=2

```{r}
set2 = RUVg(set,neg_controls,k=2)
plotRLE(set2, outline=FALSE,col=colors[group])
plotPCA(set2,col=colors[group])
```

Increasing the number of factors of unwanted variation appears to improve these plots somewhat, although now sample NA07037 looks like a bit of an outlier (particularly in the boxplot). Overall though, the distribution of the samples in the boxplot appears to be more similar than in previous steps, and separation in the PCA plot doesn't appear to be driven as much by single samples. 

```{r}
design = model.matrix(~group + W_1 + W_2, data=pData(set2))
y = DGEList(counts=counts(set2), group=group)
y = calcNormFactors(y, method="upperquartile")
y = estimateGLMCommonDisp(y, design)
y = estimateGLMTagwiseDisp(y, design)
glm_fit_d = glmFit(y, design)
glm_lrt_d = glmLRT(glm_fit_d, coef=2)$table
glm_lrt_d$PValue_FDR = p.adjust(glm_lrt_d$PValue,"fdr")
```

After controlling for two factors of unwanted variation, there are `r sum(glm_lrt_d$PValue_FDR<0.05)` genes with FDR adjusted p values < 0.05.

## e) Repeat part (d) using the RUVr method with k=2

```{r}
# GLM residuals
res = residuals(glm_fit_a, type="deviance")
# RUVr
set3 = RUVr(set,neg_controls,k=2,res)
plotRLE(set3,outline=FALSE,col=colors[group])
plotPCA(set3,col=colors[group])
```

Overall these plots look similar to using RUVg with k = 2, although in the boxplot it appears that NA11918 still has more variability than the other samples. NA11918 also seems to driving PC1, although this PCA plot is still a significant improvement over the original plot without any RUV methods applied.

```{r}
design = model.matrix(~group + W_1 + W_2, data=pData(set3))
y = DGEList(counts=counts(set3), group=group)
y = calcNormFactors(y, method="upperquartile")
y = estimateGLMCommonDisp(y, design)
y = estimateGLMTagwiseDisp(y, design)
glm_fit_e = glmFit(y, design)
glm_lrt_e = glmLRT(glm_fit_e, coef=2)$table
glm_lrt_e$PValue_FDR = p.adjust(glm_lrt_e$PValue,"fdr")
```

After controlling for two factors of unwanted variation using RUVr, there are `r sum(glm_lrt_e$PValue_FDR<0.05)` genes with FDR adjusted p values < 0.05.

## f) Concerns

My major concern about RUV methods in general is that they remove wanted variation in addition to unwanted. For example, 

# LOOK AT THE PAPER FOR LIMITATIONS

# GENERALLY WE WANT THE LOWEST POSSIBLE K THAT HELPS

# 3. Method Comparisons

Load package and data:

```{r}
library(DESeq2)
load(url("http://bowtie-bio.sourceforge.net/recount/ExpressionSets/
bottomly_eset.RData"))
bottomly.count.table = exprs(bottomly.eset)
```

## a) Create a new data frame with genes that have at least 10 counts

```{r}
# Filter
filtered = bottomly.count.table[rowSums(bottomly.count.table)>=10,]
# edgeR object
filtered_dge = DGEList(filtered)
# DESeq2 object
pheno = factor(gsub("/","_",phenoData(bottomly.eset)$strain))
filtered_dseq = DESeqDataSetFromMatrix(filtered, DataFrame(pheno),~pheno)
```

There are `r nrow(filtered)` genes with at least 10 counts across all samples.

# KEEP AN EYE OUT FOR AUTOMATIC P VALUE ADJUSTMENT

## b) Calculate the DESeq2 size factors

```{r}
filtered_dseq = estimateSizeFactors(filtered_dseq)
filtered_dge = calcNormFactors(filtered_dge,method = "TMM")
# Compare
sizes = data.frame(sizeFactors(filtered_dseq))
sizes = cbind(sizes,filtered_dge$samples$norm.factors) %>%
  rownames_to_column() 
colnames(sizes) = c("Sample","DESeq Size Factor","TMM Norm Factor")
autofit(flextable(sizes))
```

Size factors as calculated by DESeq2 are the median of the ratios of each sample over a psuedosample (the same as the RLE method in edgeR). The pseudosample is the geometric mean for each gene across all samples. The formula for the size factor of sample $s_j$ (with $i$ indexing gene) is:

$$
\hat{s_j} = \text{median}_i(\frac{k_{ij}}{(\prod_{v=1}^{k_{iv}})^{\frac{1}{m}}})
$$

The idea is to make samples which may have been sequenced at different depths more comparable. For this dataset, the size factors estimated by TMM tend to be close to 1, whereas those estimated by DESeq2 have a much larger range and are more variable. 

## c) Calculate the DESeq2 dispersions

Histograms:

```{r}
filtered_dseq = estimateDispersions(filtered_dseq,fitType = "local")
hist(dispersions(filtered_dseq),xlab = "Size Factor",
     main = "Histogram of DESeq2 Size Factors")
filtered_dge = estimateDisp(filtered_dge)
hist(filtered_dge$tagwise.dispersion,xlab = "Dispersion",
     main = "Histogram of edgeR Tagwise Dispersion")
```

Bland-Altman plot:

```{r message=FALSE}
blandr.draw(log(dispersions(filtered_dseq)),
            log(filtered_dge$tagwise.dispersion))
```

## d) Test for differences between the two strains

```{r}
filtered_dseq = nbinomWaldTest(filtered_dseq)
res = results(filtered_dseq)
design = model.matrix(~pheno)
glm_fit = glmFit(filtered_dge,design)
glm_lrt = glmLRT(glm_fit)$table
glm_lrt$PValue_BH = p.adjust(glm_lrt$PValue,"BH")
```

The DESeq2 method finds `r sum(res$padj < 0.05,na.rm = T)` genes with p values < 0.05 after Benjamini-Hochberg (BH) correction, and edgeR finds `r sum(glm_lrt$PValue_BH<0.05)`. 
