---
title: "BIOS 7659 Homework 6"
author: "Tim Vigers"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE,dpi = 600)
library(cqn)
library(edgeR)
library(biomaRt)
library(RUVSeq)
library(blandr)
library(RColorBrewer)
library(tidyverse)
library(flextable)
```

```{r}
data("montgomery.subset")
data("uCovar")
```

# 1. Differential Expression

## a) Calculate RPKM and perform a t test

RPKM stands for reads per kilobases per million reads and is calculated by $$\frac{r_g}{l_g T}=\frac{\text{# of mapped reads}}{\text{length in Kb}*\text{total mapped reads in millions}}:$$

where the total mapped reads refers to column-wise sums. 

```{r}
# Divide by column sums (in millions)
rpkm = as.data.frame(lapply(montgomery.subset, function(c){
  c / (sum(c)/1e6)
}))
# Divide rows by gene length (in Kb)
rpkm = as.matrix(rpkm) / c(uCovar$length / 1e3)
rownames(rpkm) = rownames(montgomery.subset)
```

Do a standard t test between the groups for each gene (using RPKM):

```{r}
# T test for each row
t_tests = apply(rpkm,1,function(x){
  group1 = as.numeric(x[1:5])
  group2 = as.numeric(x[6:10])
  t = t.test(group1,group2)
  return(c(t$statistic,t$p.value))
})
# Format and print
t_tests = as.data.frame(t(t_tests))
t_tests = t_tests %>% rownames_to_column() %>% 
  set_names(c("Gene","T","p")) %>% arrange(desc(abs(T))) 

t_tests %>% head(10) %>% flextable(.) %>% 
  set_caption("Top 10 Genes by t-statistic") %>% autofit(.)
```

## b) Plot the histogram of p-values

```{r}
hist(t_tests$p,main = "Histogram of p values",xlab = "p")
```

Normally we would expect a uniform distribution of p values (or hopefully one with a slight tail around 0.05, suggesting some sort of signal in the data), but this distribution has a large peak around 0.4. Looking at the genes that produce these p values, it seems that they all have 0 RPKM across most samples, and one sample with a low value. This results in an absolute value of the t statistic of 1, which corresponds to a p value of 0.374.

```{r}
t1 = t_tests$Gene[which(abs(t_tests$T)==1)]
t1 = as.data.frame(rpkm[t1,]) %>% rownames_to_column(var="Gene")
set_table_properties(flextable(head(t1,10)),
                     width = .5, layout = "autofit")
```

## c) Filter genes by total counts

```{r}
# Remove genes with low counts
filtered = montgomery.subset[rowSums(montgomery.subset)>=10,]
# Create edgeR object
filtered_dge = DGEList(filtered,group = rep(c(1,2),each=5))
```

There are a total of `r nrow(filtered)` genes with at least 10 counts across all samples. By default, the `DGEList()` function uses column sums for the `lib.size` argument. This makes the most sense for this dataset, as the provided `sizeFactors.subset` data appears to be incorrect.

## d) Calculate TMM normalization factors

```{r}
filtered_dge = calcNormFactors(filtered_dge)
autofit(flextable(filtered_dge$samples))
```

The effective library sizes are generally similar to the column sums because the normalization factors are fairly close to 1. A normalization factor > 1 increases the library size, which is similar to downscaling the counts (and vice versa for factors < 1). So, the effective library size for sample 6 is increased by about 115%. Conversely, the effective library size for sample 5 is decreased by approximately 90%, which suggests that there are a small number of high-count sequences that need to be adjusted for.

## e) Use the `estimateDisp()` function to calculate the common, trended and tagwise dispersions

```{r}
filtered_dge = estimateDisp(filtered_dge)
```

The common dispersion estimate is approximately `r round(filtered_dge$common.dispersion,3)`. Plot the tagwise dispersion estimate for each gene vs. the average log counts per million:

```{r}
plotBCV(filtered_dge)
```

The common dispersion estimate only seems to work well for a narrow range of log CPM (around 4). It appears to underestimate dispersion for the lower count genes and overestimate for the higher average count genes. Note that the y axis of the above plot is the biological CV, which is the square root of the dispersion factor. 

## f) Fit the negative binomial model

### Using the common dispersion estimate

```{r}
et = exactTest(filtered_dge,dispersion = "common")
top_common = topTags(et) %>% as.data.frame(.) %>% 
  rownames_to_column(.,var = "Gene")
autofit(flextable(top_common))
```

### Using the tag-wise dispersion estimates

```{r}
et = exactTest(filtered_dge,dispersion = "tagwise")
top_tagwise = topTags(et) %>% as.data.frame(.) %>% 
  rownames_to_column(.,var = "Gene")
autofit(flextable(top_tagwise))
```

There are only `r length(intersect(top_tagwise$Gene,top_common$Gene))` overlapping genes in the top 10 table for the two methods. Also, the top genes as determined by the common dispersion estimate approach appear to be driven more by fold change than those that are most significant using the tagwise method, because the table is essentially in decreasing order of logFC (with a couple of minor exceptions). This is because the common dispersion factor fails to correctly account for dispersion, which means that the fold change drives the significance test. The incorrect variability estimate also results in much lower p values than when using tagwise dispersion, which suggests that common dispersion probably results in more false positives.

## g) Extract the raw counts 

### For the top 10 genes based on the common dispersion

```{r}
top_common_counts = filtered_dge$counts[top_common$Gene,] %>%
  as.data.frame(.) %>% rownames_to_column(var = "Gene")
set_table_properties(flextable(top_common_counts),
                     width = .5, layout = "autofit")
```

### For the top 10 genes based on tagwise dispersion

```{r}
top_tagwise_counts = filtered_dge$counts[top_tagwise$Gene,] %>%
  as.data.frame(.) %>% rownames_to_column(var = "Gene")
set_table_properties(flextable(top_tagwise_counts),
                     width = .5, layout = "autofit")
```

The genes that are significant when using the common dispersion tend to have low counts across most samples with a few samples with high expression levels. On the other hand, the top genes based on tagwise dispersion tend to have counts more evenly spread across the samples.

### Get Ensembl information for the top genes

#### Based on common dispersion

```{r}
ensembl = useEnsembl(biomart="ensembl", dataset="hsapiens_gene_ensembl")
en = getBM(attributes=c('ensembl_gene_id','description'), 
           filters ='ensembl_gene_id', values = top_common_counts$Gene, 
           mart = ensembl)
set_table_properties(flextable(en),
                     width = .75, layout = "autofit")
```

#### Based on tagwise dispersion

```{r}
en = getBM(attributes=c('ensembl_gene_id','description'), 
           filters ='ensembl_gene_id', values = top_tagwise_counts$Gene, 
           mart = ensembl)
set_table_properties(flextable(en),
                     width = .75, layout = "autofit")
```

Almost all of the top ten genes based on common dispersion code for some sort of immunoglobulin. A few of the top tagwise genes are also immunoglobulins, but there appears to be a greater variety of function among the top tagwise genes.

# 2. Remove Unwanted Variation.

## a) Create a design matrix 

```{r}
# Fit the GLM 
group = rep(c(1,2),each=5)
design = model.matrix(~group)
filtered_dge = DGEList(filtered,group = group)
filtered_dge = calcNormFactors(filtered_dge,"upperquartile")
filtered_dge = estimateDisp(filtered_dge,design = design)
glm_fit_a = glmFit(filtered_dge,
                   dispersion = filtered_dge$tagwise.dispersion)
glm_lrt_a = glmLRT(glm_fit_a)$table
glm_lrt_a$PValue_FDR = p.adjust(glm_lrt_a$PValue,"fdr")
```

Using the likelihood ratio test, there are `r sum(glm_lrt_a$PValue_FDR<0.05)` genes with FDR-adjusted p values < 0.05.

## b) Fit edgeR models that adjust for unwanted variation

```{r}
colors = brewer.pal(3, "Set2")
set = newSeqExpressionSet(
  as.matrix(filtered),
  phenoData = data.frame(group,row.names=colnames(filtered)))
set = betweenLaneNormalization(set, which="upper")
plotRLE(set, outline=FALSE,col=colors[group])
plotPCA(set,col=colors[group])
```

Two of the samples in group 2 (NA11918 and NA12006) appear to be very different from the rest of the samples. In the boxplot these samples have a much wider range (more variability) than the others. Also, in the PCA plot there is one cluster with samples from both groups, but the two outliers from group 2 are clearly different from everything else. The first PC appears to be driven by the difference between NA12006 and the other samples, whereas PC2 seems to be driven by NA11918. Between-lane normalization alone does not appear to be sufficient for these data.

## c) Perform RUVg using negative empirical control genes

```{r}
# take the 10,000 genes with the largest likelihood ratio test 
# p-values from part a)
neg_controls = 
  rownames(head(glm_lrt_a[order(glm_lrt_a$PValue,
                                decreasing = T),],10000))
set1 = RUVg(set,neg_controls,k=1)
plotRLE(set1, outline=FALSE,col=colors[group])
plotPCA(set1,col=colors[group])
```

These plots are definitely an improvement on the previous ones. There still appear to be two samples that are different from the others (NA07037 and NA11918), but in both the boxplot and PCA plot all of the samples are closer to one another than before RUVg. 

```{r}
design = model.matrix(~group + W_1, data=pData(set1))
y = DGEList(counts=counts(set1),group = group)
y = calcNormFactors(y, method="upperquartile")
y = estimateDisp(y, design)
glm_fit_c = glmFit(y, design,dispersion = y$tagwise.dispersion)
glm_lrt_c = glmLRT(glm_fit_c, coef=2)$table
glm_lrt_c$PValue_FDR = p.adjust(glm_lrt_c$PValue,"fdr")
```

After controlling for unwanted variation, there are `r sum(glm_lrt_c$PValue_FDR<0.05)` genes with FDR adjusted p values < 0.05.

## d) Repeat part c) using k=2

```{r}
set2 = RUVg(set,neg_controls,k=2)
plotRLE(set2, outline=FALSE,col=colors[group])
plotPCA(set2,col=colors[group])
```

Increasing the number of factors of unwanted variation appears to improve these plots somewhat, although now sample NA07037 looks like a bit of an outlier (particularly in the boxplot). Overall though, the distribution of the samples in the boxplot appears to be more similar than in previous steps, and separation in the PCA plot doesn't appear to be driven as much by single samples. 

```{r}
design = model.matrix(~group + W_1 + W_2, data=pData(set2))
y = DGEList(counts=counts(set2),group = group)
y = calcNormFactors(y, method="upperquartile")
y = estimateDisp(y, design)
glm_fit_d = glmFit(y, design,dispersion = y$tagwise.dispersion)
glm_lrt_d = glmLRT(glm_fit_d, coef=2)$table
glm_lrt_d$PValue_FDR = p.adjust(glm_lrt_d$PValue,"fdr")
```

After controlling for two factors of unwanted variation, there are `r sum(glm_lrt_d$PValue_FDR<0.05)` genes with FDR adjusted p values < 0.05.

## e) Repeat part (d) using the RUVr method with k=2

```{r}
# GLM residuals
res = residuals(glm_fit_a, type="deviance")
# RUVr
set3 = RUVr(set,neg_controls,k=2,res)
plotRLE(set3,outline=FALSE,col=colors[group])
plotPCA(set3,col=colors[group])
```

Overall these plots look similar to using RUVg with k = 2, although it appears that NA11918 is still different from the other samples. NA11918 still seems to driving PC1, although this PCA plot is still a significant improvement over the original plot without any RUV methods applied.

```{r}
design = model.matrix(~group + W_1 + W_2, data=pData(set2))
y = DGEList(counts=counts(set2),group = group)
y = calcNormFactors(y, method="upperquartile")
y = estimateDisp(y, design)
glm_fit_d = glmFit(y, design,dispersion = y$tagwise.dispersion)
glm_lrt_d = glmLRT(glm_fit_d, coef=2)$table
glm_lrt_d$PValue_FDR = p.adjust(glm_lrt_d$PValue,"fdr")


design = model.matrix(~group + W_1 + W_2, data=pData(set3))
y = DGEList(counts=counts(set3),group = group)
y = calcNormFactors(y, method="upperquartile")
y = estimateDisp(y, design)
glm_fit_e = glmFit(y, design, dispersion = y$tagwise.dispersion)
glm_lrt_e = glmLRT(glm_fit_e, coef=2)$table
glm_lrt_e$PValue_FDR = p.adjust(glm_lrt_e$PValue,"fdr")
```

After controlling for two factors of unwanted variation using RUVr, there are `r sum(glm_lrt_e$PValue_FDR<0.05)` genes with FDR adjusted p values < 0.05.

## f) Concerns

Based on the diagnostic plots alone, there doesn't appear to be anything wrong with using RUV on this dataset. However, one of the important assumptions of RUVg is that there are "control" genes that are not differentially expressed. Here we chose 10,000 genes with large p values from a standard likelihood ratio test, which seems like a somewhat ad-hoc approach. It would be better if we could use prior knowledge or spike-ins to determine control genes.

The assumptions for RUVr are less strict, so I would be more inclined to move forward with that method. RUVr can use information from all the genes for normalization (although above I decided to use the same negative control genes as for RUVg), but assumes that the unwanted factors are unrelated to variables of interest. This seems like a reasonable assumption for this dataset, although it's difficult to say for sure.

The major concern I have with RUV in general is that it's easy to accidentally remove wanted variation. This isn't unique to RUV, and I think it can be avoided by carefully checking the diagnostic plots and keeping the k parameter low, but it's a concern nonetheless. 

# 3. Method Comparisons

Load package and data:

```{r}
library(DESeq2)
load(url("http://bowtie-bio.sourceforge.net/recount/ExpressionSets/bottomly_eset.RData"))
bottomly.count.table = exprs(bottomly.eset)
```

## a) Create a new data frame with genes that have at least 10 counts

```{r}
# Filter
filtered = bottomly.count.table[rowSums(bottomly.count.table)>=10,]
# DESeq2 object
pheno = factor(gsub("/","_",phenoData(bottomly.eset)$strain))
filtered_dseq = DESeqDataSetFromMatrix(filtered, DataFrame(pheno),~pheno)
# edgeR object
filtered_dge = DGEList(filtered,group = pheno)
```

There are `r nrow(filtered)` genes with at least 10 counts across all samples.

## b) Calculate the DESeq2 size factors

```{r}
filtered_dseq = estimateSizeFactors(filtered_dseq)
filtered_dge = calcNormFactors(filtered_dge,method = "TMM")
# Compare
sizes = data.frame(sizeFactors(filtered_dseq))
sizes = cbind(sizes,filtered_dge$samples$norm.factors) %>%
  rownames_to_column() 
colnames(sizes) = c("Sample","DESeq Size Factor","TMM Norm Factor")
autofit(flextable(sizes))
```

Size factors as calculated by DESeq2 are the median of the ratios of each sample over a psuedosample (the same as the RLE method in edgeR). The pseudosample is the geometric mean for each gene across all samples. The formula for the size factor of sample $s_j$ (with $i$ indexing gene) is:

$$
\hat{s_j} = \text{median}_i(\frac{k_{ij}}{(\prod_{v=1}^{k_{iv}})^{\frac{1}{m}}})
$$

The TMM approach estimates relative RNA expression using the trimmed mean of the M values, where the M value of gene g in samples k and k' is:

$$
M_g=\text{log}_2\frac{Y_{gk}/N_k}{Y_{gk'}/N_{k'}}
$$

And $Y$ indicates the observed count.

The general idea of size factors is to make samples which may have been sequenced at different depths more comparable. For this dataset, the size factors estimated by TMM tend to be close to 1, whereas those estimated by DESeq2 have a much larger range and are more variable. 

## c) Calculate the DESeq2 dispersions

Histograms of log dispersion:

```{r message=FALSE}
filtered_dseq = estimateDispersions(filtered_dseq,fitType = "local")
hist(log(dispersions(filtered_dseq)),xlab = "Size Factor",
     main = "Histogram of DESeq2 Size Factors")
filtered_dge = estimateDisp(filtered_dge)
hist(log(filtered_dge$tagwise.dispersion),xlab = "Dispersion",
     main = "Histogram of edgeR Tagwise Dispersion")
```

I found it much easier to look at the histograms of log-transformed dispersion. Based on these plots it appears that the mean dispersion estimate is similar between the two packages. However, there the DESeq estimates are distributed across a wider of range than the edgeR values. 

Bland-Altman plot:

```{r message=FALSE}
blandr.draw(log(dispersions(filtered_dseq)),
            log(filtered_dge$tagwise.dispersion))
```

The Bland-Altman (BA) plot suggests that there is not good agreement between the two methods. Here the green and red lines represent $\pm$ 1.96 SD of the differences, so we would want at least 95% of the points to lie in this range in order to be confident in the methods' agreement. The disagreement is particularly striking when the mean log dispersion is low, where some of the DESeq estimates are much larger than edgeR (the cluster in the top left of the plot), and some are a little bit smaller (the tail towards the bottom left). When the mean is in the -4 to 0 range there's less of a linear trend in the plot, but many of the points are above the green line, which indicates that DESeq's estimates are generally higher than edgeR's.

## d) Test for differences between the two strains

```{r}
filtered_dseq = nbinomWaldTest(filtered_dseq)
res = as.data.frame(results(filtered_dseq)) %>% rownames_to_column(var = "Gene")
dseq_sig = res$Gene[which(res$padj < 0.05)]

design = model.matrix(~pheno)
glm_fit = glmFit(filtered_dge,design,
                 dispersion = filtered_dge$tagwise.dispersion)
glm_lrt = as.data.frame(glmLRT(glm_fit)$table)
glm_lrt$PValue_BH = p.adjust(glm_lrt$PValue,"BH")
glm_lrt = glm_lrt %>% rownames_to_column(var = "Gene")
edger_sig = glm_lrt$Gene[which(glm_lrt$PValue_BH < 0.05)]
```

The DESeq2 method finds `r sum(res$padj < 0.05,na.rm = T)` genes with p values < 0.05 after Benjamini-Hochberg (BH) correction, and edgeR finds `r sum(glm_lrt$PValue_BH<0.05)`, with an overlap of  `r length(intersect(dseq_sig,edger_sig))` genes. Now look at the results for a few genes that were significant in the DESEq results but not significant in edgeR:

```{r}
diff = setdiff(dseq_sig,edger_sig)
dseq_diff = intersect(diff,dseq_sig)
head(res[res$Gene %in% dseq_diff,]) %>% flextable(.) %>% 
       set_caption(.,"DESeq Results") %>% 
  set_table_properties(width = .75, layout = "autofit")
head(glm_lrt[glm_lrt$Gene %in% dseq_diff,]) %>% flextable(.) %>% 
       set_caption(.,"edgeR Results") %>% set_table_properties(layout = "autofit")
```


The log fold change results are really close between the two packages, which suggests that the dispersion estimates are driving the differences we see in the significance testing. Based on this it's difficult to say which package is better. edgeR may be slightly more conservative, so it's likely a better option if you're concerned about type 1 error. However, DESeq produces more significant hits, which can be good for exploratory research which aims to discover candidate genes for future studies. These differences are fairly small though, so I think the two approaches are pretty comparable overall, despite the concerning BA plot. Also, to me neither package has an advantage in terms of usability (both are pretty good but with documentation quirks), so I think I would pick my workflow based on the question at hand.
