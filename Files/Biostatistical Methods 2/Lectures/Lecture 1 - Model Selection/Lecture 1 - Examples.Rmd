---
title: "Lecture 1: Model Selection"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R^2, sums of squares, and mean squares

We will simulate some data from a linear model with two important covariates and one unimportant covariate. We want to look at the effects of including or excluding certain covariates on R^2 and sums of squares.

```{r rsq_sims}
set.seed(1)

# sample size
n <- 200

# create three covariates
x1 <- rnorm(n)
x2 <- runif(n)
x3 <- rbinom(n,1,.5)

# true parameter vector
beta0 <- c(-1,1,0,1)

# residual variance
sigma2 <- 2^2

# get outcome
y <- cbind(1,x1,x2,x3) %*% beta0 + rnorm(n,sd=sqrt(sigma2))

# fit a model with just x1 (model 1)
mod1 <- lm(y ~ x1)

# model sum of squares is 
ss.mod1 <- sum(head(anova(mod1)[,2],-1))
#total sum of squares is
ss.tot1 <- sum(anova(mod1)[,2])
# so we can calculate R^2 as
ss.mod1/ss.tot1
# and compare with 
summary(mod1)$r.squared

# now add the (irrelevant) covariate x2 (model 2)
mod2 <- lm(y ~ x1 + x2)

# model sum of squares is 
ss.mod2 <- sum(head(anova(mod2)[,2],-1))
#total sum of squares is
ss.tot2 <- sum(anova(mod2)[,2])
# compare with model sum of squares without including x2
ss.mod1
ss.mod2

# now add the covariate x3 to the model containing only x1 (model 3)
mod3 <- lm(y ~ x1 + x3)

# model sum of squares is 
ss.mod3 <- sum(head(anova(mod3)[,2],-1))
#total sum of squares is
ss.tot3 <- sum(anova(mod3)[,2])
# compare with model sum of squares without including x2
ss.mod1
ss.mod3

# look at R^2 for each model
summary(mod1)$r.squared
summary(mod2)$r.squared
summary(mod3)$r.squared

# compare MSE for each model
tail(anova(mod1)[,3],1)
tail(anova(mod2)[,3],1)
tail(anova(mod3)[,3],1)
```

## Partial F tests
We can compare nested models with the partial F test. 

```{r partial_ftest}
# model 1 is nested within models 2 and 3, but model 2 is not nested within model 3 (for example)

# adding an unimportant covariate should not result in a significant decrease in SSE
anova(mod1,mod2)

# but adding an important covariate should
anova(mod1,mod3)

```

## AIC and BIC
If we want to compare non-nested models, we can look at information criteria. This doesn't allow for p-values like the F tests, but is available in much more general scenarios.

```{r aic_calcs}
# using the built-in function
AIC(mod1)
# can calculate this manually as well
-2*logLik(mod1)+2*(length(coef(mod1))+1)
# have to remember to add 1 parameter for the residual variance

# for the other two models
AIC(mod2)
AIC(mod3)
```
BIC is very similar, but has a larger penalty for overfitting.

```{r bic}
BIC(mod1)
BIC(mod2)
BIC(mod3)
```
The best-fitting model will have lowest AIC or BIC.

## General strategies for model selection

Let's consider now the same basic setup as before but add more "noise" variables.
```{r noise_covars}
set.seed(2)
x4 <- runif(n)
x5 <- rbinom(n,1,.25)
x6 <- rbinom(n,1,.9)
x7 <- rexp(n,1)
x8 <- rnorm(n,sd=2)
```

There is an R package that does best subsets. It gets the best model at each model size, so which is considered the best overall depends on AIC, BIC, etc., but which within each size does not.

```{r best_subsets}
library(leaps)
regsubs <- regsubsets(y ~ x1+x2+x3+x4+x5+x6+x7+x8,
                      data=data.frame(y,x1,x2,x3,x4,x5,x6,x7,x8),
                      nvmax=3)

data.frame(summary(regsubs)$outmat)

summary(regsubs)$bic
```

This correctly identifies the model with ```x1``` and ```x3``` as best-fitting model since the 2-covariate model has the lowest BIC and the best 2-covariate model includes these two covariates.

## Forward and backward selection

There is another package that allows for forward, backward, and stepwise model selection based on AIC.

```{r step_select}
library(MASS)

back.fit <- stepAIC(lm(y ~ x1+x2+x3+x4+x5+x6+x7+x8), # starting model
                    direction='backward', # specify direction of selection
                    trace=0) # suppress output during computations
forw.fit <- stepAIC(lm(y ~ x1), # starting model
                    scope=y ~ x1+x2+x3+x4+x5+x6+x7+x8, # full model for consideration
                    direction='forward',
                    trace=0)

back.fit
forw.fit
```


Both methods correctly identify the model with ```x1``` and ```x3```.

We can also force inclusion of some covariate of interest.

```{r step_force}
force.x5 <- stepAIC(lm(y ~ x1+x2+x3+x4+x5+x6+x7+x8), # starting model
                    scope=list(upper=~ x1+x2+x3+x4+x5+x6+x7+x8,
                               lower=~ x5), # say we want to force x5 into the model
                    direction='backward', 
                    trace=1)
# in tracing, we see that x5 is never considered for elimination
force.x5
```

## Real data example
Weight (```wgt```), height (```hgt```), and age (```age```) of a random sample of 12 nutritionally deficient children (KKM, p. 385). 

```{r nutr_data}
nutrition.data <- data.frame(wgt=c(64,71,53,67,55,58,77,57,56,51,76,68),
                             hgt=c(57,59,49,62,51,50,55,48,42,42,61,57),
                             age=c(8,10,6,11,8,7,10,9,10,6,12,9))

# examine the data set
nutrition.data

# fit the full or saturated model
nutr.fullfit <- lm(wgt ~ age + hgt + age:hgt + I(age^2) + I(hgt^2), data=nutrition.data,x=TRUE)

nutr.subsets <- regsubsets(wgt ~ age + hgt + age:hgt + I(age^2) + I(hgt^2), data=nutrition.data)

# calculate fit statistics for best model at each model size
nutr.bic <- summary(nutr.subsets)$bic
nutr.adjr2 <- summary(nutr.subsets)$adjr2
nutr.cp <- summary(nutr.subsets)$cp

data.frame(summary(nutr.subsets)$outmat,nutr.bic,nutr.adjr2,nutr.cp)
```

Looking at the output from the best subsets function, BIC would select the most parsimonious model, but that includes an interaction without the main effect. Sticking with models obeying the proper hierarchy, we would go with a model containing just age and height as the best fit: it has the highest adjusted R^2 and a low C_p. 

## Multicollinearity

When there is high correlation between predictors, multicollinearity may be an issue. 

```{r multicoll}

mod.hgt1 <- lm(wgt ~ hgt, data=nutrition.data)
mod.hgt2 <- lm(wgt ~ hgt + I(hgt^2), data=nutrition.data)
mod.hgt3 <- lm(wgt ~ poly(hgt,2,raw=FALSE), data=nutrition.data,x=TRUE)

orthcovs <- as.data.frame(mod.hgt3$x[,-1])
colnames(orthcovs) <- c('p1','p2')
mod.hgt4 <- lm(nutrition.data$wgt ~ orthcovs$p1+orthcovs$p2)

summary(mod.hgt3)
summary(mod.hgt4)

vif(mod.hgt2)
vif(mod.hgt4)

```

The ```poly()``` function offers a way to orthogonalize possibly collinear covariates, but does make the model fit harder to interpret.

## Outliers

We can look at diagnostic plots to check for outlying observations. We will use the example of the weight data set to see how this is done. Recall that we fit a model containing height, age, their quadratics and their interaction. 

```{r outliers_diag}
# get jackknife residuals (also called studentized residuals)
res.nutr <- rstudent(nutr.fullfit)

boxplot(res.nutr,horizontal=TRUE)
```

Since residuals beyond either +3 or -3 may be of concern, we might want to look more closely.

```{r outlier_ident}
res.nutr
```

Observation 10 has a jackknife residual of almost 3. Let's look at its leverage.

```{r outlier_leverage}
XtX <- t(nutr.fullfit$x) %*% nutr.fullfit$x # the X'X matrix

hat.mat <- nutr.fullfit$x %*% solve(XtX) %*% t(nutr.fullfit$x) # the hat matrix

plot(res.nutr,diag(hat.mat))
text(res.nutr,diag(hat.mat),pos=1,labels=1:nrow(nutrition.data))

# cbind(res.nutr,diag(hat.mat))
```

It has high leverage, but so does another data point (observation 9). Recall that high leverage doesn't necessarily mean high influence, just that there is the potential for high influence. 

We can look at influence using Cook's distance.

```{r cooks_dist}
cooks.dist <- resid(nutr.fullfit)^2*diag(hat.mat)/(length(coef(nutr.fullfit))*summary(nutr.fullfit)$sigma^2*(1-diag(hat.mat))^2)

# plot the measure of influence versus the measure of leverage
plot(diag(hat.mat),cooks.dist)
text(diag(hat.mat),cooks.dist,pos=4,labels=1:nrow(nutrition.data))
abline(h=1,lty=2)
```

We see from this plot that although observations 9 and 10 have similar leverage, only observation 10 has high influence.

Another way to look at influence is with DFFITS. This is easily calculated from the hat matrix diagonals and jackknife residuals.

```{r dffits_calc}
dffits.nutr <- res.nutr*sqrt(diag(hat.mat)/(1-diag(hat.mat)))

plot(dffits.nutr,xlab='observation number')
# the cutoff for this to be of concern is 2*sqrt((p+1)/n)
abline(h=2*sqrt(length(coef(nutr.fullfit))/nrow(nutrition.data)),lty=2)
```

This shows again that observation 10 is influential; observations 7 and 9 fall slightly above the cutoff, but not so far as to cause too much concern.

## Influence measures

R has a built-in function to calculate all these measures and a few more.

```{r inflmeas_fun}
influence.measures(nutr.fullfit)
```

The first p+1 columns of this matrix are the DFBETAS, followed by DFFITS, covariance ratios (which we haven't talked about), Cook's distance, and hat matrix diagonals.